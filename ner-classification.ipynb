{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset (2 HR, 2 SR, 4 SLO):\n",
    "\n",
    "    - For each model (XLM-R-base, XLM-R-large, CSEBert, SloBERTa, BERTić, multiple versions of XLM-R-BERTić and XLM-R-SloBERTić):\n",
    "\n",
    "\n",
    "        - fine-tune the model and evaluate it - 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import logging\n",
    "import sklearn\n",
    "from numba import cuda\n",
    "import argparse\n",
    "import gc\n",
    "import torch\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Import the dataset\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"dataset\", help=\"path to the dataset in JSON format\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = args.dataset\n",
    "\n",
    "# Load the json file\n",
    "with open(dataset_path, \"r\") as file:\n",
    "    json_dict = json.load(file)\n",
    "\n",
    "# Open the train, eval and test dictionaries as DataFrames\n",
    "train_df = pd.DataFrame(json_dict[\"train\"])\n",
    "test_df = pd.DataFrame(json_dict[\"test\"])\n",
    "dev_df = pd.DataFrame(json_dict[\"dev\"])\n",
    "\n",
    "# Change the sentence_ids to numbers\n",
    "test_df['sentence_id'] = pd.factorize(test_df['sentence_id'])[0]\n",
    "train_df['sentence_id'] = pd.factorize(train_df['sentence_id'])[0]\n",
    "dev_df['sentence_id'] = pd.factorize(dev_df['sentence_id'])[0]\n",
    "\n",
    "# Define the labels\n",
    "LABELS = json_dict[\"labels\"]\n",
    "print(LABELS)\n",
    "\n",
    "print(train_df.shape, test_df.shape, dev_df.shape)\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at models/xlmrb_bcms-12/ and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([12, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([12]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started. Current model: xlmrb_bcms-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: 'runs/Aug22_07-54-35_kt-gpu2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     83\u001b[0m \u001b[39m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m current_model\u001b[39m.\u001b[39;49mtrain_model(train_df)\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:513\u001b[0m, in \u001b[0;36mNERModel.train_model\u001b[0;34m(self, train_data, output_dir, show_running_loss, args, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m train_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_and_cache_examples(train_data)\n\u001b[1;32m    511\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 513\u001b[0m global_step, training_details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    514\u001b[0m     train_dataset,\n\u001b[1;32m    515\u001b[0m     output_dir,\n\u001b[1;32m    516\u001b[0m     show_running_loss\u001b[39m=\u001b[39;49mshow_running_loss,\n\u001b[1;32m    517\u001b[0m     eval_data\u001b[39m=\u001b[39;49meval_data,\n\u001b[1;32m    518\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    519\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model(model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m    523\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m Training of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m model complete. Saved to \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmodel_type, output_dir\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:550\u001b[0m, in \u001b[0;36mNERModel.train\u001b[0;34m(self, train_dataset, output_dir, show_running_loss, eval_data, test_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m    548\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\n\u001b[0;32m--> 550\u001b[0m tb_writer \u001b[39m=\u001b[39m SummaryWriter(log_dir\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtensorboard_dir)\n\u001b[1;32m    551\u001b[0m train_sampler \u001b[39m=\u001b[39m RandomSampler(train_dataset)\n\u001b[1;32m    552\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    553\u001b[0m     train_dataset,\n\u001b[1;32m    554\u001b[0m     sampler\u001b[39m=\u001b[39mtrain_sampler,\n\u001b[1;32m    555\u001b[0m     batch_size\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    556\u001b[0m     num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    557\u001b[0m )\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py:247\u001b[0m, in \u001b[0;36mSummaryWriter.__init__\u001b[0;34m(self, log_dir, comment, purge_step, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39m# Initialize the file writers, but they can be cleared out on close\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m# and recreated later as needed.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_file_writer()\n\u001b[1;32m    249\u001b[0m \u001b[39m# Create default bins for histograms, see generate_testdata.py in tensorflow/tensorboard\u001b[39;00m\n\u001b[1;32m    250\u001b[0m v \u001b[39m=\u001b[39m \u001b[39m1e-12\u001b[39m\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py:277\u001b[0m, in \u001b[0;36mSummaryWriter._get_file_writer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the default FileWriter instance. Recreates it if closed.\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39m=\u001b[39m FileWriter(\n\u001b[1;32m    278\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_dir, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_queue, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_secs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename_suffix\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39m=\u001b[39m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer\u001b[39m.\u001b[39mget_logdir(): \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer}\n\u001b[1;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpurge_step \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py:76\u001b[0m, in \u001b[0;36mFileWriter.__init__\u001b[0;34m(self, log_dir, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m# Sometimes PosixPath is passed in and we need to coerce it to\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m# a string in all cases\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# TODO: See if we can remove this in the future if we are\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m# actually the ones passing in a PosixPath\u001b[39;00m\n\u001b[1;32m     75\u001b[0m log_dir \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(log_dir)\n\u001b[0;32m---> 76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent_writer \u001b[39m=\u001b[39m EventFileWriter(\n\u001b[1;32m     77\u001b[0m     log_dir, max_queue, flush_secs, filename_suffix\n\u001b[1;32m     78\u001b[0m )\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/tensorboard/summary/writer/event_file_writer.py:72\u001b[0m, in \u001b[0;36mEventFileWriter.__init__\u001b[0;34m(self, logdir, max_queue_size, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a `EventFileWriter` and an event file to write to.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[39mOn construction the summary writer creates a new event file in `logdir`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m    pending events and summaries to disk.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logdir \u001b[39m=\u001b[39m logdir\n\u001b[0;32m---> 72\u001b[0m tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mgfile\u001b[39m.\u001b[39;49mmakedirs(logdir)\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file_name \u001b[39m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     75\u001b[0m         logdir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39m+\u001b[39m filename_suffix\n\u001b[1;32m     85\u001b[0m )  \u001b[39m# noqa E128\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_general_file_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mGFile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file_name, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:907\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmakedirs\u001b[39m(path):\n\u001b[1;32m    900\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Creates a directory and all parent/intermediate directories.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \n\u001b[1;32m    902\u001b[0m \u001b[39m    It succeeds if path already exists and is writable.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[39m      path: string, name of the directory to be created\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m     \u001b[39mreturn\u001b[39;00m get_filesystem(path)\u001b[39m.\u001b[39;49mmakedirs(path)\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:208\u001b[0m, in \u001b[0;36mLocalFileSystem.makedirs\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmakedirs\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Creates a directory and all parent/intermediate directories.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(path, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: 'runs/Aug22_07-54-35_kt-gpu2'"
     ]
    }
   ],
   "source": [
    "model = 'xlmrb_bcms-12'\n",
    "\n",
    "# Define the model\n",
    "\n",
    "# Define the model arguments - use the same one as for XLM-R-large if model is based on it,\n",
    "# if the model is of same size as XLM-R-base, use its optimal hyperparameters (I searched for them before)\n",
    "xlm_r_large_args = {\"overwrite_output_dir\": True,\n",
    "            \"num_train_epochs\": 5,\n",
    "            \"labels_list\": LABELS,\n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"train_batch_size\": 32,\n",
    "            # Comment out no_cache and no_save if you want to save the model\n",
    "            \"no_cache\": True,\n",
    "            \"no_save\": True,\n",
    "            \"max_seq_length\": 256,\n",
    "            \"save_steps\": -1,\n",
    "            \"silent\": True,\n",
    "            }\n",
    "\n",
    "xlm_r_base_args = {\"overwrite_output_dir\": True,\n",
    "            \"num_train_epochs\": 9,\n",
    "            \"labels_list\": LABELS,\n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"train_batch_size\": 32,\n",
    "            # Comment out no_cache and no_save if you want to save the model\n",
    "            \"no_cache\": True,\n",
    "            \"no_save\": True,\n",
    "            \"max_seq_length\": 256,\n",
    "            \"save_steps\": -1,\n",
    "        \"silent\": True,\n",
    "            }\n",
    "\n",
    "\n",
    "# Model type - a dictionary of type and model name.\n",
    "# To refer to our own models, use the path to the model directory as the model name.\n",
    "model_type_dict = {\n",
    "    \"sloberta\": [\"camembert\", \"EMBEDDIA/sloberta\", xlm_r_base_args],\n",
    "    \"csebert\": [\"bert\", \"EMBEDDIA/crosloengual-bert\", xlm_r_base_args],\n",
    "    \"xlm-r-base\": [\"xlmroberta\", \"xlm-roberta-base\", xlm_r_base_args],\n",
    "    \"xlm-r-large\": [\"xlmroberta\", \"xlm-roberta-large\", xlm_r_large_args],\n",
    "    \"bertic\": [\"electra\", \"classla/bcms-bertic\", xlm_r_base_args]\n",
    "}\n",
    "\n",
    "if \"xlmrb\" in model:\n",
    "    model_args = xlm_r_base_args\n",
    "\n",
    "    # Define the model\n",
    "    current_model = NERModel(\n",
    "    \"xlmroberta\",\n",
    "    \"models/{}/\".format(model),\n",
    "    labels = LABELS,\n",
    "    use_cuda=True,\n",
    "    args = model_args,\n",
    "    ignore_mismatched_sizes=True)\n",
    "\n",
    "elif \"xlmrl\" in model:\n",
    "    model_args = xlm_r_large_args\n",
    "\n",
    "    # Define the model\n",
    "    current_model = NERModel(\n",
    "    \"xlmroberta\",\n",
    "    \"models/{}/\".format(model),\n",
    "    labels = LABELS,\n",
    "    use_cuda=True,\n",
    "    args = model_args,\n",
    "    ignore_mismatched_sizes=True)\n",
    "\n",
    "else:\n",
    "    # Update the hyperparameters accordingly to the model\n",
    "    model_args = model_type_dict[model][2]\n",
    "\n",
    "    # Define the model\n",
    "    current_model = NERModel(\n",
    "    model_type_dict[model][0],\n",
    "    model_type_dict[model][1],\n",
    "    labels = LABELS,\n",
    "    use_cuda=True,\n",
    "    args = model_args)\n",
    "\n",
    "print(\"Training started. Current model: {}\".format(model))\n",
    "start_time = time.time()\n",
    "\n",
    "# Fine-tune the model\n",
    "current_model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_df, test_df, dataset_path, LABELS):\n",
    "\n",
    "    # Define the model\n",
    "\n",
    "    # Define the model arguments - use the same one as for XLM-R-large if model is based on it,\n",
    "    # if the model is of same size as XLM-R-base, use its optimal hyperparameters (I searched for them before)\n",
    "    xlm_r_large_args = {\"overwrite_output_dir\": True,\n",
    "                \"num_train_epochs\": 5,\n",
    "                \"labels_list\": LABELS,\n",
    "                \"learning_rate\": 1e-5,\n",
    "                \"train_batch_size\": 32,\n",
    "                # Comment out no_cache and no_save if you want to save the model\n",
    "                \"no_cache\": True,\n",
    "                \"no_save\": True,\n",
    "                \"max_seq_length\": 256,\n",
    "                \"save_steps\": -1,\n",
    "                \"silent\": True,\n",
    "                }\n",
    "\n",
    "    xlm_r_base_args = {\"overwrite_output_dir\": True,\n",
    "             \"num_train_epochs\": 9,\n",
    "             \"labels_list\": LABELS,\n",
    "             \"learning_rate\": 1e-5,\n",
    "             \"train_batch_size\": 32,\n",
    "             # Comment out no_cache and no_save if you want to save the model\n",
    "             \"no_cache\": True,\n",
    "             \"no_save\": True,\n",
    "             \"max_seq_length\": 256,\n",
    "             \"save_steps\": -1,\n",
    "            \"silent\": True,\n",
    "             }\n",
    "\n",
    "\n",
    "    # Model type - a dictionary of type and model name.\n",
    "    # To refer to our own models, use the path to the model directory as the model name.\n",
    "    model_type_dict = {\n",
    "        \"sloberta\": [\"camembert\", \"EMBEDDIA/sloberta\", xlm_r_base_args],\n",
    "        \"csebert\": [\"bert\", \"EMBEDDIA/crosloengual-bert\", xlm_r_base_args],\n",
    "        \"xlm-r-base\": [\"xlmroberta\", \"xlm-roberta-base\", xlm_r_base_args],\n",
    "        \"xlm-r-large\": [\"xlmroberta\", \"xlm-roberta-large\", xlm_r_large_args],\n",
    "        \"bertic\": [\"electra\", \"classla/bcms-bertic\", xlm_r_base_args]\n",
    "    }\n",
    "\n",
    "    if \"xlmrb\" in model:\n",
    "        model_args = xlm_r_base_args\n",
    "\n",
    "        # Define the model\n",
    "        current_model = NERModel(\n",
    "        \"xlmroberta\",\n",
    "        \"/models/{}\".format(model),\n",
    "        labels = LABELS,\n",
    "        use_cuda=True,\n",
    "        args = model_args)\n",
    "\n",
    "    elif \"xlmrl\" in model:\n",
    "        model_args = xlm_r_large_args\n",
    "\n",
    "        # Define the model\n",
    "        current_model = NERModel(\n",
    "        \"xlmroberta\",\n",
    "        \"/models/{}\".format(model),\n",
    "        labels = LABELS,\n",
    "        use_cuda=True,\n",
    "        args = model_args)\n",
    "\n",
    "    else:\n",
    "        # Update the hyperparameters accordingly to the model\n",
    "        model_args = model_type_dict[model][2]\n",
    "\n",
    "        # Define the model\n",
    "        current_model = NERModel(\n",
    "        model_type_dict[model][0],\n",
    "        model_type_dict[model][1],\n",
    "        labels = LABELS,\n",
    "        use_cuda=True,\n",
    "        args = model_args)\n",
    "\n",
    "    print(\"Training started. Current model: {}\".format(model))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fine-tune the model\n",
    "    current_model.train_model(train_df)\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    training_time = round((time.time() - start_time)/60,2)\n",
    "\n",
    "    print(\"It took {} minutes for {} instances.\".format(training_time, train_df.shape[0]))\n",
    "\n",
    "    # Clean cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    start_evaluation_time = time.time()\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = current_model.eval_model(test_df)\n",
    "\n",
    "    print(\"Evaluation completed.\")\n",
    "\n",
    "    evaluation_time = round((time.time() - start_evaluation_time)/60,2)\n",
    "\n",
    "    print(\"It took {} minutes for {} instances.\".format(evaluation_time, test_df.shape[0]))\n",
    "\n",
    "    # Get predictions\n",
    "    preds = results[1]\n",
    "\n",
    "    # Create a list with predictions\n",
    "    preds_list = []\n",
    "\n",
    "    for sentence in preds:\n",
    "        for word in sentence:\n",
    "            current_word = []\n",
    "            for element in word:\n",
    "                # Find prediction with the highest value\n",
    "                highest_index = element.index(max(element))\n",
    "                # Transform the index to label\n",
    "                current_pred = current_model.config.id2label[highest_index]\n",
    "                # Append to the list\n",
    "                current_word.append(current_pred)\n",
    "            # Segmentation can result in multiple predictions for one word - use the first prediction only\n",
    "            preds_list.append(current_word[0])\n",
    "    \n",
    "    # Get y_true\n",
    "    y_true = list(test_df.labels)\n",
    "\n",
    "    run_name = \"{}-{}\".format(dataset_path, model)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    metrics = evaluate.testing(y_true, preds_list, list(test_df.labels.unique()), run_name, show_matrix=True)\n",
    "\n",
    "    # Add y_pred and y_true to the metrics dict\n",
    "    metrics[\"y_true\"] = y_true\n",
    "    metrics[\"y_pred\"] = preds_list\n",
    "\n",
    "    # Let's also add entire results\n",
    "    metrics[\"results_output\"] = results    \n",
    "    \n",
    "    # The function returns a dict with accuracy, micro f1, macro f1, y_true and y_pred\n",
    "    return metrics\n",
    "\n",
    "# For each model, repeat training and testing 5 times - let's do 2 times for starters\n",
    "#model_list = [\"xlm-r-large\", \"sloberta\", \"csebert\", \"xlm-r-base\", \"bertic\"]\n",
    "model_list = ['xlmrb_bcms-12','xlmrb_bcms-24', 'xlmrb_bcms-36', 'xlmrb_bcms-48', 'xlmrb_bcms-60', 'xlmrb_bcms-72', 'xlmrb_bcms-84', 'xlmrb_bcms-96', 'xlmrl_bcms-6', 'xlmrl_bcms-12', 'xlmrl_bcms-18', 'xlmrl_bcms-24', 'xlmrl_bcms-30', 'xlmrl_bcms-36', 'xlmrl_bcms-42', 'xlmrl_bcms-48', 'xlmrl_sl-bcms-6']\n",
    "\n",
    "for model in model_list:\n",
    "    for run in list(range(2)):\n",
    "        current_results_dict = train_and_test(model, train_df, test_df, dataset_path, LABELS)\n",
    "\n",
    "        # Add to the dict model name, dataset name and run\n",
    "        current_results_dict[\"model\"] = model\n",
    "        current_results_dict[\"run\"] = \"{}-{}\".format(model, run)\n",
    "        current_results_dict[\"dataset\"] = dataset_path\n",
    "\n",
    "        # Add to the file with results all important information\n",
    "        with open(\"ner-results.txt\", \"a\") as file:\n",
    "            file.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), current_results_dict[\"model\"], current_results_dict[\"run\"], current_results_dict[\"dataset\"], current_results_dict[\"micro F1\"], current_results_dict[\"macro F1\"], current_results_dict[\"label-report\"]))\n",
    "\n",
    "        # Add to the original test_df y_preds\n",
    "        test_df[\"y_pred_{}_{}\".format(model, run)] = current_results_dict[\"y_pred\"]\n",
    "\n",
    "        # Save also y_pred and y_true\n",
    "        with open(\"logs/{}-{}-{}-true-and-pred-backlog.txt\".format(dataset_path,model,run), \"w\") as backlog:\n",
    "            backlog.write(\"y-true\\ty-pred\\toutputs\\n\")\n",
    "            backlog.write(\"{}\\t{}\\t{}\\n\".format(current_results_dict[\"y_true\"], current_results_dict[\"y_pred\"], current_results_dict[\"results_output\"]))\n",
    "    \n",
    "        print(\"Run {} finished.\".format(run))\n",
    "\n",
    "# At the end, save the test_df with all predictions\n",
    "test_df.to_csv(\"{}-test_df-with-predictions.csv\".format(dataset_path))\n",
    "\n",
    "# At the end, create a csv table with a summary of results\n",
    "\n",
    "results = pd.read_csv(\"ner-results.txt\", sep=\"\\t\")\n",
    "\n",
    "results[\"Macro F1\"] = results[\"Macro F1\"].round(2)\n",
    "\n",
    "# Pivot the DataFrame to rearrange columns into rows\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dataset Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "# Define the gpu on the gpu machine\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import logging\n",
    "import sklearn\n",
    "from numba import cuda\n",
    "import argparse\n",
    "import gc\n",
    "import torch\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-loc', 'B-org', 'B-per', 'I-per', 'B-deriv-per', 'I-org', 'I-loc', 'B-misc', 'I-misc', 'I-deriv-per']\n",
      "(398681, 3) (51190, 3) (49764, 3)\n",
      "     sentence_id      words labels\n",
      "717            0      Kazna      O\n",
      "718            0  medijskom      O\n",
      "719            0     mogulu      O\n",
      "720            0   obnovila      O\n",
      "721            0   raspravu      O\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset\n",
    "\n",
    "# Code for python script\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"dataset\", help=\"path to the dataset in JSON format\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = args.dataset\n",
    "\"\"\"\n",
    "# Define the path to the dataset\n",
    "dataset_path = \"datasets/hr500k.conllup_extracted.json\"\n",
    "\n",
    "# Load the json file\n",
    "with open(dataset_path, \"r\") as file:\n",
    "    json_dict = json.load(file)\n",
    "\n",
    "# Open the train, eval and test dictionaries as DataFrames\n",
    "train_df = pd.DataFrame(json_dict[\"train\"])\n",
    "test_df = pd.DataFrame(json_dict[\"test\"])\n",
    "dev_df = pd.DataFrame(json_dict[\"dev\"])\n",
    "\n",
    "# Change the sentence_ids to numbers\n",
    "test_df['sentence_id'] = pd.factorize(test_df['sentence_id'])[0]\n",
    "train_df['sentence_id'] = pd.factorize(train_df['sentence_id'])[0]\n",
    "dev_df['sentence_id'] = pd.factorize(dev_df['sentence_id'])[0]\n",
    "\n",
    "# Define the labels\n",
    "LABELS = json_dict[\"labels\"]\n",
    "print(LABELS)\n",
    "\n",
    "print(train_df.shape, test_df.shape, dev_df.shape)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_df, test_df, dataset_path, LABELS):\n",
    "\n",
    "    # Define the model\n",
    "\n",
    "    # Define the model arguments - use the same one as for XLM-R-large if model is based on it,\n",
    "    # if the model is of same size as XLM-R-base, use its optimal hyperparameters (I searched for them before)\n",
    "    xlm_r_large_args = {\"overwrite_output_dir\": True,\n",
    "                \"num_train_epochs\": 5,\n",
    "                \"labels_list\": LABELS,\n",
    "                \"learning_rate\": 1e-5,\n",
    "                \"train_batch_size\": 32,\n",
    "                # Comment out no_cache and no_save if you want to save the model\n",
    "                \"no_cache\": True,\n",
    "                \"no_save\": True,\n",
    "                \"max_seq_length\": 256,\n",
    "                \"save_steps\": -1,\n",
    "                \"silent\": True,\n",
    "                }\n",
    "\n",
    "    xlm_r_base_args = {\"overwrite_output_dir\": True,\n",
    "             \"num_train_epochs\": 9,\n",
    "             \"labels_list\": LABELS,\n",
    "             \"learning_rate\": 1e-5,\n",
    "             \"train_batch_size\": 32,\n",
    "             # Comment out no_cache and no_save if you want to save the model\n",
    "             \"no_cache\": True,\n",
    "             \"no_save\": True,\n",
    "             \"max_seq_length\": 256,\n",
    "             \"save_steps\": -1,\n",
    "            \"silent\": True,\n",
    "             }\n",
    "\n",
    "\n",
    "    # Model type - a dictionary of type and model name.\n",
    "    # To refer to our own models, use the path to the model directory as the model name.\n",
    "    model_type_dict = {\n",
    "        \"sloberta\": [\"camembert\", \"EMBEDDIA/sloberta\", xlm_r_base_args],\n",
    "        \"csebert\": [\"bert\", \"EMBEDDIA/crosloengual-bert\", xlm_r_base_args],\n",
    "        \"xlm-r-base\": [\"xlmroberta\", \"xlm-roberta-base\", xlm_r_base_args],\n",
    "        \"xlm-r-large\": [\"xlmroberta\", \"xlm-roberta-large\", xlm_r_large_args],\n",
    "        \"bertic\": [\"electra\", \"classla/bcms-bertic\", xlm_r_base_args],\n",
    "        \"xlmrb_bcms_12\": [\"xlmroberta\", \"models/xlmrb_bcms_12\", xlm_r_base_args],\n",
    "        \"xlmrl_bcms_48000\": [\"xlmroberta\", \"output\", xlm_r_large_args]\n",
    "    }\n",
    "\n",
    "    # Update the hyperparameters accordingly to the model\n",
    "    model_args = model_type_dict[model][2]\n",
    "\n",
    "    if \"bcms\" in model:\n",
    "        model_path = model_type_dict[model][1]\n",
    "        model_args[\"output_dir\"] = \"models/{}/\".format(model)\n",
    "        model_args[\"no_save\"] = False\n",
    "        model_args[\"num_train_epoch\"] = 1\n",
    "\n",
    "    # Define the model\n",
    "    current_model = NERModel(\n",
    "    model_type_dict[model][0],\n",
    "    model_type_dict[model][1],\n",
    "    labels = LABELS,\n",
    "    use_cuda=True,\n",
    "    args = model_args)\n",
    "\n",
    "    print(\"Training started. Current model: {}\".format(model))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fine-tune the model\n",
    "    current_model.train_model(train_df)\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    training_time = round((time.time() - start_time)/60,2)\n",
    "\n",
    "    print(\"It took {} minutes for {} instances.\".format(training_time, train_df.shape[0]))\n",
    "\n",
    "    # Clean cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    start_evaluation_time = time.time()\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = current_model.eval_model(test_df)\n",
    "\n",
    "    print(\"Evaluation completed.\")\n",
    "\n",
    "    evaluation_time = round((time.time() - start_evaluation_time)/60,2)\n",
    "\n",
    "    print(\"It took {} minutes for {} instances.\".format(evaluation_time, test_df.shape[0]))\n",
    "\n",
    "    # Get predictions\n",
    "    preds = results[1]\n",
    "\n",
    "    # Create a list with predictions\n",
    "    preds_list = []\n",
    "\n",
    "    for sentence in preds:\n",
    "        for word in sentence:\n",
    "            current_word = []\n",
    "            for element in word:\n",
    "                # Find prediction with the highest value\n",
    "                highest_index = element.index(max(element))\n",
    "                # Transform the index to label\n",
    "                current_pred = current_model.config.id2label[highest_index]\n",
    "                # Append to the list\n",
    "                current_word.append(current_pred)\n",
    "            # Segmentation can result in multiple predictions for one word - use the first prediction only\n",
    "            preds_list.append(current_word[0])\n",
    "    \n",
    "    # Get y_true\n",
    "    y_true = list(test_df.labels)\n",
    "\n",
    "    run_name = \"{}-{}\".format(dataset_path, model)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    metrics = evaluate.testing(y_true, preds_list, list(test_df.labels.unique()), run_name, show_matrix=True)\n",
    "\n",
    "    # Add y_pred and y_true to the metrics dict\n",
    "    metrics[\"y_true\"] = y_true\n",
    "    metrics[\"y_pred\"] = preds_list\n",
    "\n",
    "    # Let's also add entire results\n",
    "    metrics[\"results_output\"] = results    \n",
    "    \n",
    "    # The function returns a dict with accuracy, micro f1, macro f1, y_true and y_pred\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/cache/nikolal/xlmrl_bcms_exp/checkpoint-6000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-12000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-18000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-24000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-30000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-36000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-42000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-48000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-6000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-12000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-18000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-24000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-30000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-42000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-48000']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create lists of all needed models for the task\n",
    "base_dict = {\"/cache/nikolal/xlmrb_bcms_exp/checkpoint-12000\": \"xlmrb_bcms-12\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-24000\": \"xlmrb_bcms-24\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-36000\": \"xlmrb_bcms-36\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-48000\": \"xlmrb_bcms-48\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-60000\": \"xlmrb_bcms-60\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-72000\": \"xlmrb_bcms-72\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-84000\": \"xlmrb_bcms-84\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-96000\": \"xlmrb_bcms-96\"}\n",
    "large_dict = {\"/cache/nikolal/xlmrl_bcms_exp/checkpoint-6000\": \"xlmrl_bcms-6\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-12000\":\"xlmrl_bcms-12\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-18000\": \"xlmrl_bcms-18\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-24000\": \"xlmrl_bcms-24\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-30000\": \"xlmrl_bcms-30\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-36000\": \"xlmrl_bcms-36\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-42000\": \"xlmrl_bcms-42\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-48000\": \"xlmrl_bcms-48\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-6000\": \"xlmrl_sl-bcms-6\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-12000\": \"xlmrl_sl-bcms-12\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-18000\": \"xlmrl_sl-bcms-18\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-24000\": \"xlmrl_sl-bcms-24\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-30000\": \"xlmrl_sl-bcms-30\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-42000\": \"xlmrl_sl-bcms-42\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-48000\": \"xlmrl_sl-bcms-48\"}\n",
    "\n",
    "base_list = list(base_dict.keys())\n",
    "large_list = list(large_dict.keys())\n",
    "large_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xlmrl_bcms-6', 'xlmrl_bcms-12', 'xlmrl_bcms-18', 'xlmrl_bcms-24', 'xlmrl_bcms-30', 'xlmrl_bcms-36', 'xlmrl_bcms-42', 'xlmrl_bcms-48', 'xlmrl_sl-bcms-6', 'xlmrl_sl-bcms-12', 'xlmrl_sl-bcms-18', 'xlmrl_sl-bcms-24', 'xlmrl_sl-bcms-30', 'xlmrl_sl-bcms-42', 'xlmrl_sl-bcms-48']\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "print(list(large_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_checkpoint(model_path, model_size, train_df, test_df, dataset_path, LABELS):\n",
    "\t# When fine-tuning our custom models that we pre-trained, and using them from checkpoints, the process is a bit different than with publicly available models: first, we need to fine-tune a model from the original checkpoint, so that we save the model and overwrite its original settings which force pretraining from a specific step (and disable fine-tuning by that). Then we take that new model and fine-tune it, as we did with the models before. \n",
    "\n",
    "\t# Create lists of all needed models for the task\n",
    "\tpath_list = {\"/cache/nikolal/xlmrb_bcms_exp/checkpoint-12000\": \"xlmrb_bcms-12\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-24000\": \"xlmrb_bcms-24\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-36000\": \"xlmrb_bcms-36\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-48000\": \"xlmrb_bcms-48\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-60000\": \"xlmrb_bcms-60\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-72000\": \"xlmrb_bcms-72\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-84000\": \"xlmrb_bcms-84\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-96000\": \"xlmrb_bcms-96\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-6000\": \"xlmrl_bcms-6\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-12000\":\"xlmrl_bcms-12\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-18000\": \"xlmrl_bcms-18\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-24000\": \"xlmrl_bcms-24\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-30000\": \"xlmrl_bcms-30\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-36000\": \"xlmrl_bcms-36\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-42000\": \"xlmrl_bcms-42\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-48000\": \"xlmrl_bcms-48\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-6000\": \"xlmrl_sl-bcms-6\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-12000\": \"xlmrl_sl-bcms-12\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-18000\": \"xlmrl_sl-bcms-18\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-24000\": \"xlmrl_sl-bcms-24\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-30000\": \"xlmrl_sl-bcms-30\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-42000\": \"xlmrl_sl-bcms-42\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-48000\": \"xlmrl_sl-bcms-48\"}\n",
    "\n",
    "\t# Define the model arguments - use the same one as for XLM-R-large if model is based on it,\n",
    "\t# if the model is of same size as XLM-R-base, use its optimal hyperparameters (I searched for them before)\n",
    "\txlm_r_large_args = {\"overwrite_output_dir\": True,\n",
    "\t\t\t\"num_train_epochs\": 5,\n",
    "\t\t\t\"labels_list\": LABELS,\n",
    "\t\t\t\"learning_rate\": 1e-5,\n",
    "\t\t\t\"train_batch_size\": 32,\n",
    "\t\t\t# Comment out no_cache and no_save if you want to save the model\n",
    "\t\t\t\"no_cache\": True,\n",
    "\t\t\t\"no_save\": True,\n",
    "\t\t\t\"max_seq_length\": 256,\n",
    "\t\t\t\"save_steps\": -1,\n",
    "\t\t\t\"silent\": True,\n",
    "\t\t\t}\n",
    "\n",
    "\txlm_r_base_args = {\"overwrite_output_dir\": True,\n",
    "\t\t\t\"num_train_epochs\": 9,\n",
    "\t\t\t\"labels_list\": LABELS,\n",
    "\t\t\t\"learning_rate\": 1e-5,\n",
    "\t\t\t\"train_batch_size\": 32,\n",
    "\t\t\t# Comment out no_cache and no_save if you want to save the model\n",
    "\t\t\t\"no_cache\": True,\n",
    "\t\t\t\"no_save\": True,\n",
    "\t\t\t\"max_seq_length\": 256,\n",
    "\t\t\t\"save_steps\": -1,\n",
    "\t\t\t\"silent\": True,\n",
    "\t\t\t}\n",
    "\t\n",
    "\tif model_size == \"base\":\n",
    "\t\t# Update the hyperparameters accordingly to the model\n",
    "\t\tmodel_args = xlm_r_base_args\n",
    "\telif model_size == \"large\":\n",
    "\t\tmodel_args = xlm_r_large_args\n",
    "\n",
    "\t# Add additional arguments, specific for our own models\n",
    "\t# Specify the folder where we want to save the models\n",
    "\tnew_model_path = path_list[model_path]\n",
    "\tmodel_args[\"output_dir\"] = \"models/{}/\".format(new_model_path)\n",
    "\tmodel_args[\"no_save\"] = False\n",
    "\tmodel_args[\"num_train_epoch\"] = 1\n",
    "\n",
    "\t# Define the model\n",
    "\tcurrent_model = NERModel(\n",
    "\t\"xlmroberta\",\n",
    "\tmodel_path,\n",
    "\tlabels = LABELS,\n",
    "\tuse_cuda=True,\n",
    "\targs = model_args)\n",
    "\n",
    "\tprint(\"Training started. Current model: {}\".format(model))\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\t# Fine-tune the model\n",
    "\tcurrent_model.train_model(train_df)\n",
    "\n",
    "\tprint(\"Training completed.\")\n",
    "\n",
    "\tprint(\"Model saved as models/{}/\".format(new_model_path))\n",
    "\n",
    "\t# Clean cache\n",
    "\tgc.collect()\n",
    "\ttorch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\t#start_evaluation_time = time.time()\n",
    "\n",
    "\t# Evaluate the model\n",
    "\t#results = current_model.eval_model(test_df)\n",
    "\n",
    "\t#print(\"Evaluation completed.\")\n",
    "\n",
    "\t#evaluation_time = round((time.time() - start_evaluation_time)/60,2)\n",
    "\n",
    "\t#print(\"It took {} minutes for {} instances.\".format(evaluation_time, test_df.shape[0]))\n",
    "\n",
    "\t# Get predictions\n",
    "\t#preds = results[1]\n",
    "\n",
    "\t# Create a list with predictions\n",
    "\t#preds_list = []\n",
    "\n",
    "\tfor sentence in preds:\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tcurrent_word = []\n",
    "\t\t\tfor element in word:\n",
    "\t\t\t\t# Find prediction with the highest value\n",
    "\t\t\t\thighest_index = element.index(max(element))\n",
    "\t\t\t\t# Transform the index to label\n",
    "\t\t\t\tcurrent_pred = current_model.config.id2label[highest_index]\n",
    "\t\t\t\t# Append to the list\n",
    "\t\t\t\tcurrent_word.append(current_pred)\n",
    "\t\t\t# Segmentation can result in multiple predictions for one word - use the first prediction only\n",
    "\t\t\tpreds_list.append(current_word[0])\n",
    "\n",
    "\t# Get y_true\n",
    "\ty_true = list(test_df.labels)\n",
    "\n",
    "\trun_name = \"{}-{}\".format(dataset_path, model)\n",
    "\n",
    "\t# Evaluate predictions\n",
    "\tmetrics = evaluate.testing(y_true, preds_list, list(test_df.labels.unique()), run_name, show_matrix=True)\n",
    "\n",
    "\t# Add y_pred and y_true to the metrics dict\n",
    "\tmetrics[\"y_true\"] = y_true\n",
    "\tmetrics[\"y_pred\"] = preds_list\n",
    "\n",
    "\t# Let's also add entire results\n",
    "\tmetrics[\"results_output\"] = results    \n",
    "\n",
    "\t# The function returns a dict with accuracy, micro f1, macro f1, y_true and y_pred\n",
    "\treturn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started. Current model: xlmrb_bcms_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995f89ec217e4b598352ce9c4b935df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5e375baa1c41a6914a68c9acf68c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c57dc8df9ea469bb5f1c5c995522f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 9:   0%|          | 0/619 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633c386a1f91472aafd1c3b09f805678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 9:   0%|          | 0/619 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model_list \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mxlmrb_bcms_12\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m model_list:\n\u001b[0;32m----> 5\u001b[0m \tcurrent_results_dict \u001b[39m=\u001b[39m train_and_test(model, train_df, test_df, dataset_path, LABELS)\n\u001b[1;32m      7\u001b[0m \t\u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRun finished.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 70\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model, train_df, test_df, dataset_path, LABELS)\u001b[0m\n\u001b[1;32m     67\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     69\u001b[0m \u001b[39m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m current_model\u001b[39m.\u001b[39;49mtrain_model(train_df)\n\u001b[1;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining completed.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m training_time \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m((time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time)\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m,\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:513\u001b[0m, in \u001b[0;36mNERModel.train_model\u001b[0;34m(self, train_data, output_dir, show_running_loss, args, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m train_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_and_cache_examples(train_data)\n\u001b[1;32m    511\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 513\u001b[0m global_step, training_details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    514\u001b[0m     train_dataset,\n\u001b[1;32m    515\u001b[0m     output_dir,\n\u001b[1;32m    516\u001b[0m     show_running_loss\u001b[39m=\u001b[39;49mshow_running_loss,\n\u001b[1;32m    517\u001b[0m     eval_data\u001b[39m=\u001b[39;49meval_data,\n\u001b[1;32m    518\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    519\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model(model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m    523\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m Training of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m model complete. Saved to \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmodel_type, output_dir\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:804\u001b[0m, in \u001b[0;36mNERModel.train\u001b[0;34m(self, train_dataset, output_dir, show_running_loss, eval_data, test_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    800\u001b[0m     loss \u001b[39m=\u001b[39m (\n\u001b[1;32m    801\u001b[0m         loss\u001b[39m.\u001b[39mmean()\n\u001b[1;32m    802\u001b[0m     )  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m current_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m show_running_loss:\n\u001b[1;32m    807\u001b[0m     batch_iterator\u001b[39m.\u001b[39mset_description(\n\u001b[1;32m    808\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpochs \u001b[39m\u001b[39m{\u001b[39;00mepoch_number\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39mnum_train_epochs\u001b[39m}\u001b[39;00m\u001b[39m. Running Loss: \u001b[39m\u001b[39m{\u001b[39;00mcurrent_loss\u001b[39m:\u001b[39;00m\u001b[39m9.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    809\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing the models if they work as expected\n",
    "model_list = [\"xlmrb_bcms_12\"]\n",
    "\n",
    "for model in model_list:\n",
    "\tcurrent_results_dict = train_and_test(model, train_df, test_df, dataset_path, LABELS)\n",
    "\n",
    "\tprint(\"Run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_list = dir(NERArgs)\n",
    "\n",
    "import torch\n",
    "\n",
    "optimizer_state = torch.load(\"model/checkpoint-48000/training_args.bin\")\n",
    "\n",
    "attributes = list(dir(optimizer_state))\n",
    "\n",
    "\n",
    "# Find the intersection of the sets\n",
    "common_elements = list(set(attributes).intersection(set(ner_list)))\n",
    "\n",
    "print(common_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer_state.resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_state.warmup_ratio = 0.06\n",
    "optimizer_state.learning_rate = 1e-5\n",
    "optimizer_state.fp16 = True\n",
    "optimizer_state.logging_steps = 50\n",
    "#optimizer_state.n_gpu = 1\n",
    "optimizer_state.gradient_accumulation_steps = 1\n",
    "optimizer_state.output_dir = \"outputs/\"\n",
    "optimizer_state.num_train_epochs = 1\n",
    "optimizer_state.resume_from_checkpoint = True\n",
    "optimizer_state.ignore_data_skip = True\n",
    "\n",
    "\n",
    "# Save arguments with new attributes\n",
    "torch.save(optimizer_state, \"model/checkpoint-48000/training_args.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new file for results\n",
    "#with open(\"ner-results.txt\", \"w\") as file:\n",
    "#    file.write(\"Date\\tModel\\tRun\\tDataset\\tMicro F1\\tMacro F1\\tLabel Report\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the models if they work as expected\n",
    "# models: [\"xlm-r-large\", \"sloberta\", \"csebert\", \"xlm-r-base\", \"bertic\"]\n",
    "model = \"xlmrl-bcms-48\"\n",
    "run = \"test\"\n",
    "\n",
    "current_results_dict = train_and_test(model, train_df, test_df, dataset_path)\n",
    "\n",
    "# Add to the dict model name, dataset name and run\n",
    "current_results_dict[\"model\"] = model\n",
    "current_results_dict[\"run\"] = \"{}-{}\".format(model, run)\n",
    "current_results_dict[\"dataset\"] = dataset_path\n",
    "\n",
    "# Add to the file with results all important information\n",
    "#with open(\"ner-results-testing.txt\", \"a\") as file:\n",
    "#    file.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), current_results_dict[\"model\"], current_results_dict[\"run\"], current_results_dict[\"dataset\"], current_results_dict[\"micro F1\"], current_results_dict[\"macro F1\"], current_results_dict[\"Micro F1 Nikola\"], current_results_dict[\"Macro F1 Nikola\"], current_results_dict[\"label-report\"]))\n",
    "\n",
    "# Add to the original test_df y_preds\n",
    "#test_df[\"y_pred_{}_{}\".format(model, run)] = current_results_dict[\"y_pred\"]\n",
    "\n",
    "# Save entire dict just in case\n",
    "#with open(\"{}-{}-{}-backlog.json\".format(dataset_path,model,run), \"w\") as backlog:\n",
    "#    json.dump(current_results_dict, backlog, indent=2)\n",
    "\n",
    "print(\"Run {} finished.\".format(run))\n",
    "\n",
    "# At the end, save the test_df with all predictions\n",
    "#test_df.to_csv(\"{}-test_df-with-predictions.csv\".format(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the txt with results\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.read_csv(\"ner-results.txt\", sep=\"\\t\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.Dataset.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average micro and macro F1\n",
    "dataset = \"datasets/reldi-normtagner-sr.conllup_extracted.json\"\n",
    "\n",
    "# Define the dataset to inspect\n",
    "import matplotlib as plt\n",
    "results[results[\"Dataset\"] == dataset].groupby(\"Model\")[\"Micro F1\"].mean().plot(kind=\"line\", color=\"blue\")\n",
    "results[results[\"Dataset\"] == dataset].groupby(\"Model\")[\"Macro F1\"].mean().plot(kind=\"line\",color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results[\"Dataset\"] == dataset].groupby(\"Model\")[\"Macro F1\"].mean().round(2)\n",
    "\n",
    "results[\"Macro F1\"] = results[\"Macro F1\"].round(2)\n",
    "\n",
    "# Pivot the DataFrame to rearrange columns into rows\n",
    "pivot_df = results.pivot(index='Run', columns='Dataset', values='Macro F1')\n",
    "\n",
    "# Rename the columns\n",
    "pivot_df.columns = list(results.Dataset.unique())\n",
    "\n",
    "# Reset the index to have 'Model' as a column\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the df with all the predictions\n",
    "import numpy as np\n",
    "\n",
    "pred_df = pd.read_csv(\"datasets/hr500k.conllup_extracted.json-test_df-with-predictions.csv\", index_col = 0)\n",
    "\n",
    "# Analyze instances where models are wrong\n",
    "pred_df[\"match\"] = np.where(pred_df[\"labels\"] != pred_df[\"y_pred_xlm-r-large_0\"], \"no\", \"yes\")\n",
    "pred_df.match.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[pred_df[\"match\"] == \"no\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
