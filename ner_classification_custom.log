Some weights of the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-12000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-12000 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
['O', 'B-loc', 'B-org', 'B-per', 'I-per', 'B-deriv-per', 'I-org', 'I-loc', 'B-misc', 'I-misc']
(74259, 3) (11421, 3) (11993, 3)
     sentence_id      words labels
726            0      Kazna      O
727            0  medijskom      O
728            0     mogulu      O
729            0   obnovila      O
730            0     debatu      O
Training of pre-trained model started. Current model: /cache/nikolal/xlmrb_bcms_exp/checkpoint-12000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 115
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 12000
INFO:simpletransformers.ner.ner_model:   Will skip the first 40 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrb_bcms-12
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.2 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.0569075065132123, 'precision': 0.9386845039018952, 'recall': 0.9345172031076582, 'f1_score': 0.9365962180200221}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.2 minutes for 11421 instances.
Macro f1: 0.903, Micro f1: 0.989
Accuracy: 0.989
Run 0 finished.
Training started. Current model: xlmrb_bcms-12
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.1 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06778335233729404, 'precision': 0.9388888888888889, 'recall': 0.9378468368479467, 'f1_score': 0.9383675735702387}
Some weights of the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-24000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-24000 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.22 minutes for 11421 instances.
Macro f1: 0.909, Micro f1: 0.99
Accuracy: 0.99
Run 1 finished.
Training of pre-trained model started. Current model: /cache/nikolal/xlmrb_bcms_exp/checkpoint-24000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 230
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 24000
INFO:simpletransformers.ner.ner_model:   Will skip the first 80 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrb_bcms-24
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.1 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.045296611028830876, 'precision': 0.9411764705882353, 'recall': 0.9411764705882353, 'f1_score': 0.9411764705882353}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.22 minutes for 11421 instances.
Macro f1: 0.929, Micro f1: 0.991
Accuracy: 0.991
Run 0 finished.
Training started. Current model: xlmrb_bcms-24
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.11 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05733698795721956, 'precision': 0.9488888888888889, 'recall': 0.9478357380688124, 'f1_score': 0.9483620210993893}
Some weights of the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-36000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-36000 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.22 minutes for 11421 instances.
Macro f1: 0.938, Micro f1: 0.992
Accuracy: 0.992
Run 1 finished.
Training of pre-trained model started. Current model: /cache/nikolal/xlmrb_bcms_exp/checkpoint-36000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 346
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 36000
INFO:simpletransformers.ner.ner_model:   Will skip the first 16 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrb_bcms-36
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.1 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.049980258882099476, 'precision': 0.9315673289183223, 'recall': 0.9367369589345172, 'f1_score': 0.9341449916989486}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.22 minutes for 11421 instances.
Macro f1: 0.911, Micro f1: 0.989
Accuracy: 0.989
Run 0 finished.
Training started. Current model: xlmrb_bcms-36
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.12 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06246559972211029, 'precision': 0.9446290143964563, 'recall': 0.946725860155383, 'f1_score': 0.9456762749445677}
Some weights of the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-48000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-48000 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.22 minutes for 11421 instances.
Macro f1: 0.929, Micro f1: 0.991
Accuracy: 0.991
Run 1 finished.
Training of pre-trained model started. Current model: /cache/nikolal/xlmrb_bcms_exp/checkpoint-48000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 461
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 48000
INFO:simpletransformers.ner.ner_model:   Will skip the first 56 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrb_bcms-48
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.08 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.04949585384053465, 'precision': 0.9273927392739274, 'recall': 0.9356270810210877, 'f1_score': 0.9314917127071825}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.23 minutes for 11421 instances.
Macro f1: 0.906, Micro f1: 0.99
Accuracy: 0.99
Run 0 finished.
Training started. Current model: xlmrb_bcms-48
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.13 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06083818016133871, 'precision': 0.938257993384785, 'recall': 0.9445061043285239, 'f1_score': 0.9413716814159292}
Some weights of the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-60000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-60000 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.23 minutes for 11421 instances.
Macro f1: 0.932, Micro f1: 0.991
Accuracy: 0.991
Run 1 finished.
Training of pre-trained model started. Current model: /cache/nikolal/xlmrb_bcms_exp/checkpoint-60000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 576
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 60000
INFO:simpletransformers.ner.ner_model:   Will skip the first 96 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrb_bcms-60
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.13 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.049961100685043044, 'precision': 0.9244249726177437, 'recall': 0.9367369589345172, 'f1_score': 0.9305402425578831}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.22 minutes for 11421 instances.
Macro f1: 0.909, Micro f1: 0.989
Accuracy: 0.989
Run 0 finished.
Training started. Current model: xlmrb_bcms-60
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.12 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06346099509962816, 'precision': 0.9362637362637363, 'recall': 0.9456159822419534, 'f1_score': 0.9409166206515737}
Some weights of the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-72000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-72000 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.24 minutes for 11421 instances.
Macro f1: 0.919, Micro f1: 0.991
Accuracy: 0.991
Run 1 finished.
Training of pre-trained model started. Current model: /cache/nikolal/xlmrb_bcms_exp/checkpoint-72000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 692
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 72000
INFO:simpletransformers.ner.ner_model:   Will skip the first 32 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrb_bcms-72
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.12 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05146552610654348, 'precision': 0.9413716814159292, 'recall': 0.9445061043285239, 'f1_score': 0.9429362880886426}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.23 minutes for 11421 instances.
Macro f1: 0.923, Micro f1: 0.991
Accuracy: 0.991
Run 0 finished.
Training started. Current model: xlmrb_bcms-72
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.13 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06701366500688331, 'precision': 0.9425414364640884, 'recall': 0.946725860155383, 'f1_score': 0.9446290143964562}
Some weights of the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-84000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-84000 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.23 minutes for 11421 instances.
Macro f1: 0.923, Micro f1: 0.991
Accuracy: 0.991
Run 1 finished.
Training of pre-trained model started. Current model: /cache/nikolal/xlmrb_bcms_exp/checkpoint-84000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 807
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 84000
INFO:simpletransformers.ner.ner_model:   Will skip the first 72 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrb_bcms-84
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.13 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.04982183193295406, 'precision': 0.9446290143964563, 'recall': 0.946725860155383, 'f1_score': 0.9456762749445677}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.23 minutes for 11421 instances.
Macro f1: 0.93, Micro f1: 0.991
Accuracy: 0.991
Run 0 finished.
Training started. Current model: xlmrb_bcms-84
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.13 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.0649029148511405, 'precision': 0.9401993355481728, 'recall': 0.9422863485016648, 'f1_score': 0.9412416851441242}
Some weights of the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-96000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrb_bcms_exp/checkpoint-96000 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.24 minutes for 11421 instances.
Macro f1: 0.925, Micro f1: 0.99
Accuracy: 0.99
Run 1 finished.
Training of pre-trained model started. Current model: /cache/nikolal/xlmrb_bcms_exp/checkpoint-96000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 923
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 96000
INFO:simpletransformers.ner.ner_model:   Will skip the first 8 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrb_bcms-96
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.14 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05051887453524754, 'precision': 0.9425414364640884, 'recall': 0.946725860155383, 'f1_score': 0.9446290143964562}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.24 minutes for 11421 instances.
Macro f1: 0.923, Micro f1: 0.991
Accuracy: 0.991
Run 0 finished.
Training started. Current model: xlmrb_bcms-96
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 3.12 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.061776187714489283, 'precision': 0.946725860155383, 'recall': 0.946725860155383, 'f1_score': 0.946725860155383}
Evaluation completed.
It took 0.24 minutes for 11421 instances.
Macro f1: 0.928, Micro f1: 0.991
Accuracy: 0.991
Run 1 finished.
