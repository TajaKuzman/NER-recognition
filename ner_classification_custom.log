Some weights of the model checkpoint at /cache/nikolal/xlmrl_bcms_exp/checkpoint-6000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrl_bcms_exp/checkpoint-6000 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
['O', 'B-loc', 'B-org', 'B-per', 'I-per', 'B-deriv-per', 'I-org', 'I-loc', 'B-misc', 'I-misc', 'I-deriv-per']
(398681, 3) (51190, 3) (49764, 3)
     sentence_id      words labels
717            0      Kazna      O
718            0  medijskom      O
719            0     mogulu      O
720            0   obnovila      O
721            0   raspravu      O
Training of pre-trained model started. Current model: /cache/nikolal/xlmrl_bcms_exp/checkpoint-6000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 9
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 6000
INFO:simpletransformers.ner.ner_model:   Will skip the first 429 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrl_bcms-6
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 13.96 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.053575070784167976, 'precision': 0.8978819969742814, 'recall': 0.916248552682362, 'f1_score': 0.9069723018147088}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.52 minutes for 51190 instances.
Macro f1: 0.918, Micro f1: 0.99
Accuracy: 0.99
Run 0 finished.
Training started. Current model: xlmrl_bcms-6
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 14.03 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05159881043511187, 'precision': 0.9073652239939256, 'recall': 0.9224237746043998, 'f1_score': 0.9148325358851673}
Some weights of the model checkpoint at /cache/nikolal/xlmrl_bcms_exp/checkpoint-12000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrl_bcms_exp/checkpoint-12000 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.59 minutes for 51190 instances.
Macro f1: 0.923, Micro f1: 0.991
Accuracy: 0.991
Run 1 finished.
Training of pre-trained model started. Current model: /cache/nikolal/xlmrl_bcms_exp/checkpoint-12000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 19
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 12000
INFO:simpletransformers.ner.ner_model:   Will skip the first 239 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrl_bcms-12
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 14.11 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.04960862891573433, 'precision': 0.9074423876086135, 'recall': 0.9270551910459283, 'f1_score': 0.917143948071783}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.62 minutes for 51190 instances.
Macro f1: 0.927, Micro f1: 0.991
Accuracy: 0.991
Run 0 finished.
Training started. Current model: xlmrl_bcms-12
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 14.11 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.04805154709971947, 'precision': 0.9081942336874052, 'recall': 0.9239675800849093, 'f1_score': 0.9160130093744021}
Some weights of the model checkpoint at /cache/nikolal/xlmrl_bcms_exp/checkpoint-18000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrl_bcms_exp/checkpoint-18000 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.61 minutes for 51190 instances.
Macro f1: 0.924, Micro f1: 0.991
Accuracy: 0.991
Run 1 finished.
Training of pre-trained model started. Current model: /cache/nikolal/xlmrl_bcms_exp/checkpoint-18000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 29
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 18000
INFO:simpletransformers.ner.ner_model:   Will skip the first 49 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrl_bcms-18
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 14.04 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05282631914189604, 'precision': 0.8971751412429378, 'recall': 0.9193361636433809, 'f1_score': 0.9081204727411362}
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.58 minutes for 51190 instances.
Macro f1: 0.922, Micro f1: 0.99
Accuracy: 0.99
Run 0 finished.
Training started. Current model: xlmrl_bcms-18
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 13.96 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05026133122939012, 'precision': 0.9011320754716982, 'recall': 0.9216518718641451, 'f1_score': 0.911276473955352}
Some weights of the model checkpoint at /cache/nikolal/xlmrl_bcms_exp/checkpoint-24000 were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /cache/nikolal/xlmrl_bcms_exp/checkpoint-24000 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.59 minutes for 51190 instances.
Macro f1: 0.926, Micro f1: 0.99
Accuracy: 0.99
Run 1 finished.
Training of pre-trained model started. Current model: /cache/nikolal/xlmrl_bcms_exp/checkpoint-24000
INFO:simpletransformers.ner.ner_model:   Continuing training from checkpoint, will skip to saved global_step
INFO:simpletransformers.ner.ner_model:   Continuing training from epoch 38
INFO:simpletransformers.ner.ner_model:   Continuing training from global step 24000
INFO:simpletransformers.ner.ner_model:   Will skip the first 478 steps in the current epoch
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to models/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training of pre-trained model completed.
Model saved in models/
Training started. Current model: xlmrl_bcms-24
INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.
