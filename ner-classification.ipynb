{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset (2 HR, 2 SR, 4 SLO):\n",
    "\n",
    "    - For each model (XLM-R-base, XLM-R-large, CSEBert, SloBERTa, BERTić, multiple versions of XLM-R-BERTić and XLM-R-SloBERTić):\n",
    "\n",
    "\n",
    "        - fine-tune the model and evaluate it - 5 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dataset Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "# Define the gpu on the gpu machine\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import logging\n",
    "import sklearn\n",
    "from numba import cuda\n",
    "import argparse\n",
    "import gc\n",
    "import torch\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-loc', 'B-org', 'B-per', 'I-per', 'B-deriv-per', 'I-org', 'I-loc', 'B-misc', 'I-misc', 'I-deriv-per']\n",
      "(398681, 3) (51190, 3) (49764, 3)\n",
      "     sentence_id      words labels\n",
      "717            0      Kazna      O\n",
      "718            0  medijskom      O\n",
      "719            0     mogulu      O\n",
      "720            0   obnovila      O\n",
      "721            0   raspravu      O\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset\n",
    "\n",
    "# Code for python script\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"dataset\", help=\"path to the dataset in JSON format\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = args.dataset\n",
    "\"\"\"\n",
    "# Define the path to the dataset\n",
    "dataset_path = \"datasets/hr500k.conllup_extracted.json\"\n",
    "\n",
    "# Load the json file\n",
    "with open(dataset_path, \"r\") as file:\n",
    "    json_dict = json.load(file)\n",
    "\n",
    "# Open the train, eval and test dictionaries as DataFrames\n",
    "train_df = pd.DataFrame(json_dict[\"train\"])\n",
    "test_df = pd.DataFrame(json_dict[\"test\"])\n",
    "dev_df = pd.DataFrame(json_dict[\"dev\"])\n",
    "\n",
    "# Change the sentence_ids to numbers\n",
    "test_df['sentence_id'] = pd.factorize(test_df['sentence_id'])[0]\n",
    "train_df['sentence_id'] = pd.factorize(train_df['sentence_id'])[0]\n",
    "dev_df['sentence_id'] = pd.factorize(dev_df['sentence_id'])[0]\n",
    "\n",
    "# Define the labels\n",
    "LABELS = json_dict[\"labels\"]\n",
    "print(LABELS)\n",
    "\n",
    "print(train_df.shape, test_df.shape, dev_df.shape)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_df, test_df, dataset_path, LABELS):\n",
    "\n",
    "    # Define the model\n",
    "\n",
    "    # Define the model arguments - use the same one as for XLM-R-large if model is based on it,\n",
    "    # if the model is of same size as XLM-R-base, use its optimal hyperparameters (I searched for them before)\n",
    "    xlm_r_large_args = {\"overwrite_output_dir\": True,\n",
    "                \"num_train_epochs\": 5,\n",
    "                \"labels_list\": LABELS,\n",
    "                \"learning_rate\": 1e-5,\n",
    "                \"train_batch_size\": 32,\n",
    "                # Comment out no_cache and no_save if you want to save the model\n",
    "                \"no_cache\": True,\n",
    "                \"no_save\": True,\n",
    "                \"max_seq_length\": 256,\n",
    "                \"save_steps\": -1,\n",
    "                \"silent\": True,\n",
    "                }\n",
    "\n",
    "    xlm_r_base_args = {\"overwrite_output_dir\": True,\n",
    "             \"num_train_epochs\": 9,\n",
    "             \"labels_list\": LABELS,\n",
    "             \"learning_rate\": 1e-5,\n",
    "             \"train_batch_size\": 32,\n",
    "             # Comment out no_cache and no_save if you want to save the model\n",
    "             \"no_cache\": True,\n",
    "             \"no_save\": True,\n",
    "             \"max_seq_length\": 256,\n",
    "             \"save_steps\": -1,\n",
    "            \"silent\": True,\n",
    "             }\n",
    "\n",
    "\n",
    "    # Model type - a dictionary of type and model name.\n",
    "    # To refer to our own models, use the path to the model directory as the model name.\n",
    "    model_type_dict = {\n",
    "        \"sloberta\": [\"camembert\", \"EMBEDDIA/sloberta\", xlm_r_base_args],\n",
    "        \"csebert\": [\"bert\", \"EMBEDDIA/crosloengual-bert\", xlm_r_base_args],\n",
    "        \"xlm-r-base\": [\"xlmroberta\", \"xlm-roberta-base\", xlm_r_base_args],\n",
    "        \"xlm-r-large\": [\"xlmroberta\", \"xlm-roberta-large\", xlm_r_large_args],\n",
    "        \"bertic\": [\"electra\", \"classla/bcms-bertic\", xlm_r_base_args],\n",
    "        \"xlmrb_bcms_12\": [\"xlmroberta\", \"models/xlmrb_bcms_12\", xlm_r_base_args],\n",
    "        \"xlmrl_bcms_48000\": [\"xlmroberta\", \"output\", xlm_r_large_args]\n",
    "    }\n",
    "\n",
    "    # Update the hyperparameters accordingly to the model\n",
    "    model_args = model_type_dict[model][2]\n",
    "\n",
    "    if \"bcms\" in model:\n",
    "        model_path = model_type_dict[model][1]\n",
    "        model_args[\"output_dir\"] = \"models/{}/\".format(model)\n",
    "        model_args[\"no_save\"] = False\n",
    "        model_args[\"num_train_epoch\"] = 1\n",
    "\n",
    "    # Define the model\n",
    "    current_model = NERModel(\n",
    "    model_type_dict[model][0],\n",
    "    model_type_dict[model][1],\n",
    "    labels = LABELS,\n",
    "    use_cuda=True,\n",
    "    args = model_args)\n",
    "\n",
    "    print(\"Training started. Current model: {}\".format(model))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fine-tune the model\n",
    "    current_model.train_model(train_df)\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    training_time = round((time.time() - start_time)/60,2)\n",
    "\n",
    "    print(\"It took {} minutes for {} instances.\".format(training_time, train_df.shape[0]))\n",
    "\n",
    "    # Clean cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    start_evaluation_time = time.time()\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = current_model.eval_model(test_df)\n",
    "\n",
    "    print(\"Evaluation completed.\")\n",
    "\n",
    "    evaluation_time = round((time.time() - start_evaluation_time)/60,2)\n",
    "\n",
    "    print(\"It took {} minutes for {} instances.\".format(evaluation_time, test_df.shape[0]))\n",
    "\n",
    "    # Get predictions\n",
    "    preds = results[1]\n",
    "\n",
    "    # Create a list with predictions\n",
    "    preds_list = []\n",
    "\n",
    "    for sentence in preds:\n",
    "        for word in sentence:\n",
    "            current_word = []\n",
    "            for element in word:\n",
    "                # Find prediction with the highest value\n",
    "                highest_index = element.index(max(element))\n",
    "                # Transform the index to label\n",
    "                current_pred = current_model.config.id2label[highest_index]\n",
    "                # Append to the list\n",
    "                current_word.append(current_pred)\n",
    "            # Segmentation can result in multiple predictions for one word - use the first prediction only\n",
    "            preds_list.append(current_word[0])\n",
    "    \n",
    "    # Get y_true\n",
    "    y_true = list(test_df.labels)\n",
    "\n",
    "    run_name = \"{}-{}\".format(dataset_path, model)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    metrics = evaluate.testing(y_true, preds_list, list(test_df.labels.unique()), run_name, show_matrix=True)\n",
    "\n",
    "    # Add y_pred and y_true to the metrics dict\n",
    "    metrics[\"y_true\"] = y_true\n",
    "    metrics[\"y_pred\"] = preds_list\n",
    "\n",
    "    # Let's also add entire results\n",
    "    metrics[\"results_output\"] = results    \n",
    "    \n",
    "    # The function returns a dict with accuracy, micro f1, macro f1, y_true and y_pred\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/cache/nikolal/xlmrl_bcms_exp/checkpoint-6000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-12000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-18000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-24000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-30000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-36000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-42000',\n",
       " '/cache/nikolal/xlmrl_bcms_exp/checkpoint-48000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-6000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-12000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-18000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-24000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-30000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-42000',\n",
       " '/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-48000']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create lists of all needed models for the task\n",
    "base_dict = {\"/cache/nikolal/xlmrb_bcms_exp/checkpoint-12000\": \"xlmrb_bcms-12\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-24000\": \"xlmrb_bcms-24\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-36000\": \"xlmrb_bcms-36\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-48000\": \"xlmrb_bcms-48\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-60000\": \"xlmrb_bcms-60\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-72000\": \"xlmrb_bcms-72\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-84000\": \"xlmrb_bcms-84\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-96000\": \"xlmrb_bcms-96\"}\n",
    "large_dict = {\"/cache/nikolal/xlmrl_bcms_exp/checkpoint-6000\": \"xlmrl_bcms-6\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-12000\":\"xlmrl_bcms-12\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-18000\": \"xlmrl_bcms-18\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-24000\": \"xlmrl_bcms-24\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-30000\": \"xlmrl_bcms-30\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-36000\": \"xlmrl_bcms-36\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-42000\": \"xlmrl_bcms-42\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-48000\": \"xlmrl_bcms-48\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-6000\": \"xlmrl_sl-bcms-6\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-12000\": \"xlmrl_sl-bcms-12\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-18000\": \"xlmrl_sl-bcms-18\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-24000\": \"xlmrl_sl-bcms-24\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-30000\": \"xlmrl_sl-bcms-30\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-42000\": \"xlmrl_sl-bcms-42\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-48000\": \"xlmrl_sl-bcms-48\"}\n",
    "\n",
    "base_list = list(base_dict.keys())\n",
    "large_list = list(large_dict.keys())\n",
    "large_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xlmrl_bcms-6', 'xlmrl_bcms-12', 'xlmrl_bcms-18', 'xlmrl_bcms-24', 'xlmrl_bcms-30', 'xlmrl_bcms-36', 'xlmrl_bcms-42', 'xlmrl_bcms-48', 'xlmrl_sl-bcms-6', 'xlmrl_sl-bcms-12', 'xlmrl_sl-bcms-18', 'xlmrl_sl-bcms-24', 'xlmrl_sl-bcms-30', 'xlmrl_sl-bcms-42', 'xlmrl_sl-bcms-48']\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "print(list(large_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_checkpoint(model_path, model_size, train_df, test_df, dataset_path, LABELS):\n",
    "\t# When fine-tuning our custom models that we pre-trained, and using them from checkpoints, the process is a bit different than with publicly available models: first, we need to fine-tune a model from the original checkpoint, so that we save the model and overwrite its original settings which force pretraining from a specific step (and disable fine-tuning by that). Then we take that new model and fine-tune it, as we did with the models before. \n",
    "\n",
    "\t# Create lists of all needed models for the task\n",
    "\tpath_list = {\"/cache/nikolal/xlmrb_bcms_exp/checkpoint-12000\": \"xlmrb_bcms-12\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-24000\": \"xlmrb_bcms-24\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-36000\": \"xlmrb_bcms-36\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-48000\": \"xlmrb_bcms-48\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-60000\": \"xlmrb_bcms-60\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-72000\": \"xlmrb_bcms-72\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-84000\": \"xlmrb_bcms-84\", \"/cache/nikolal/xlmrb_bcms_exp/checkpoint-96000\": \"xlmrb_bcms-96\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-6000\": \"xlmrl_bcms-6\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-12000\":\"xlmrl_bcms-12\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-18000\": \"xlmrl_bcms-18\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-24000\": \"xlmrl_bcms-24\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-30000\": \"xlmrl_bcms-30\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-36000\": \"xlmrl_bcms-36\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-42000\": \"xlmrl_bcms-42\", \"/cache/nikolal/xlmrl_bcms_exp/checkpoint-48000\": \"xlmrl_bcms-48\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-6000\": \"xlmrl_sl-bcms-6\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-12000\": \"xlmrl_sl-bcms-12\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-18000\": \"xlmrl_sl-bcms-18\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-24000\": \"xlmrl_sl-bcms-24\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-30000\": \"xlmrl_sl-bcms-30\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-42000\": \"xlmrl_sl-bcms-42\", \"/cache/nikolal/xlmrl_sl-bcms_exp/checkpoint-48000\": \"xlmrl_sl-bcms-48\"}\n",
    "\n",
    "\t# Define the model arguments - use the same one as for XLM-R-large if model is based on it,\n",
    "\t# if the model is of same size as XLM-R-base, use its optimal hyperparameters (I searched for them before)\n",
    "\txlm_r_large_args = {\"overwrite_output_dir\": True,\n",
    "\t\t\t\"num_train_epochs\": 5,\n",
    "\t\t\t\"labels_list\": LABELS,\n",
    "\t\t\t\"learning_rate\": 1e-5,\n",
    "\t\t\t\"train_batch_size\": 32,\n",
    "\t\t\t# Comment out no_cache and no_save if you want to save the model\n",
    "\t\t\t\"no_cache\": True,\n",
    "\t\t\t\"no_save\": True,\n",
    "\t\t\t\"max_seq_length\": 256,\n",
    "\t\t\t\"save_steps\": -1,\n",
    "\t\t\t\"silent\": True,\n",
    "\t\t\t}\n",
    "\n",
    "\txlm_r_base_args = {\"overwrite_output_dir\": True,\n",
    "\t\t\t\"num_train_epochs\": 9,\n",
    "\t\t\t\"labels_list\": LABELS,\n",
    "\t\t\t\"learning_rate\": 1e-5,\n",
    "\t\t\t\"train_batch_size\": 32,\n",
    "\t\t\t# Comment out no_cache and no_save if you want to save the model\n",
    "\t\t\t\"no_cache\": True,\n",
    "\t\t\t\"no_save\": True,\n",
    "\t\t\t\"max_seq_length\": 256,\n",
    "\t\t\t\"save_steps\": -1,\n",
    "\t\t\t\"silent\": True,\n",
    "\t\t\t}\n",
    "\t\n",
    "\tif model_size == \"base\":\n",
    "\t\t# Update the hyperparameters accordingly to the model\n",
    "\t\tmodel_args = xlm_r_base_args\n",
    "\telif model_size == \"large\":\n",
    "\t\tmodel_args = xlm_r_large_args\n",
    "\n",
    "\t# Add additional arguments, specific for our own models\n",
    "\t# Specify the folder where we want to save the models\n",
    "\tnew_model_path = path_list[model_path]\n",
    "\tmodel_args[\"output_dir\"] = \"models/{}/\".format(new_model_path)\n",
    "\tmodel_args[\"no_save\"] = False\n",
    "\tmodel_args[\"num_train_epoch\"] = 1\n",
    "\n",
    "\t# Define the model\n",
    "\tcurrent_model = NERModel(\n",
    "\t\"xlmroberta\",\n",
    "\tmodel_path,\n",
    "\tlabels = LABELS,\n",
    "\tuse_cuda=True,\n",
    "\targs = model_args)\n",
    "\n",
    "\tprint(\"Training started. Current model: {}\".format(model))\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\t# Fine-tune the model\n",
    "\tcurrent_model.train_model(train_df)\n",
    "\n",
    "\tprint(\"Training completed.\")\n",
    "\n",
    "\tprint(\"Model saved as models/{}/\".format(new_model_path))\n",
    "\n",
    "\t# Clean cache\n",
    "\tgc.collect()\n",
    "\ttorch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\t#start_evaluation_time = time.time()\n",
    "\n",
    "\t# Evaluate the model\n",
    "\t#results = current_model.eval_model(test_df)\n",
    "\n",
    "\t#print(\"Evaluation completed.\")\n",
    "\n",
    "\t#evaluation_time = round((time.time() - start_evaluation_time)/60,2)\n",
    "\n",
    "\t#print(\"It took {} minutes for {} instances.\".format(evaluation_time, test_df.shape[0]))\n",
    "\n",
    "\t# Get predictions\n",
    "\t#preds = results[1]\n",
    "\n",
    "\t# Create a list with predictions\n",
    "\t#preds_list = []\n",
    "\n",
    "\tfor sentence in preds:\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tcurrent_word = []\n",
    "\t\t\tfor element in word:\n",
    "\t\t\t\t# Find prediction with the highest value\n",
    "\t\t\t\thighest_index = element.index(max(element))\n",
    "\t\t\t\t# Transform the index to label\n",
    "\t\t\t\tcurrent_pred = current_model.config.id2label[highest_index]\n",
    "\t\t\t\t# Append to the list\n",
    "\t\t\t\tcurrent_word.append(current_pred)\n",
    "\t\t\t# Segmentation can result in multiple predictions for one word - use the first prediction only\n",
    "\t\t\tpreds_list.append(current_word[0])\n",
    "\n",
    "\t# Get y_true\n",
    "\ty_true = list(test_df.labels)\n",
    "\n",
    "\trun_name = \"{}-{}\".format(dataset_path, model)\n",
    "\n",
    "\t# Evaluate predictions\n",
    "\tmetrics = evaluate.testing(y_true, preds_list, list(test_df.labels.unique()), run_name, show_matrix=True)\n",
    "\n",
    "\t# Add y_pred and y_true to the metrics dict\n",
    "\tmetrics[\"y_true\"] = y_true\n",
    "\tmetrics[\"y_pred\"] = preds_list\n",
    "\n",
    "\t# Let's also add entire results\n",
    "\tmetrics[\"results_output\"] = results    \n",
    "\n",
    "\t# The function returns a dict with accuracy, micro f1, macro f1, y_true and y_pred\n",
    "\treturn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started. Current model: xlmrb_bcms_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995f89ec217e4b598352ce9c4b935df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5e375baa1c41a6914a68c9acf68c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model:   Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c57dc8df9ea469bb5f1c5c995522f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 9:   0%|          | 0/619 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633c386a1f91472aafd1c3b09f805678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 9:   0%|          | 0/619 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model_list \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mxlmrb_bcms_12\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m model_list:\n\u001b[0;32m----> 5\u001b[0m \tcurrent_results_dict \u001b[39m=\u001b[39m train_and_test(model, train_df, test_df, dataset_path, LABELS)\n\u001b[1;32m      7\u001b[0m \t\u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRun finished.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 70\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model, train_df, test_df, dataset_path, LABELS)\u001b[0m\n\u001b[1;32m     67\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     69\u001b[0m \u001b[39m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m current_model\u001b[39m.\u001b[39;49mtrain_model(train_df)\n\u001b[1;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining completed.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m training_time \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m((time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time)\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m,\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:513\u001b[0m, in \u001b[0;36mNERModel.train_model\u001b[0;34m(self, train_data, output_dir, show_running_loss, args, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m train_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_and_cache_examples(train_data)\n\u001b[1;32m    511\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 513\u001b[0m global_step, training_details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    514\u001b[0m     train_dataset,\n\u001b[1;32m    515\u001b[0m     output_dir,\n\u001b[1;32m    516\u001b[0m     show_running_loss\u001b[39m=\u001b[39;49mshow_running_loss,\n\u001b[1;32m    517\u001b[0m     eval_data\u001b[39m=\u001b[39;49meval_data,\n\u001b[1;32m    518\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    519\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model(model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m    523\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m Training of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m model complete. Saved to \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmodel_type, output_dir\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:804\u001b[0m, in \u001b[0;36mNERModel.train\u001b[0;34m(self, train_dataset, output_dir, show_running_loss, eval_data, test_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    800\u001b[0m     loss \u001b[39m=\u001b[39m (\n\u001b[1;32m    801\u001b[0m         loss\u001b[39m.\u001b[39mmean()\n\u001b[1;32m    802\u001b[0m     )  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m current_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m show_running_loss:\n\u001b[1;32m    807\u001b[0m     batch_iterator\u001b[39m.\u001b[39mset_description(\n\u001b[1;32m    808\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpochs \u001b[39m\u001b[39m{\u001b[39;00mepoch_number\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39mnum_train_epochs\u001b[39m}\u001b[39;00m\u001b[39m. Running Loss: \u001b[39m\u001b[39m{\u001b[39;00mcurrent_loss\u001b[39m:\u001b[39;00m\u001b[39m9.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    809\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing the models if they work as expected\n",
    "model_list = [\"xlmrb_bcms_12\"]\n",
    "\n",
    "for model in model_list:\n",
    "\tcurrent_results_dict = train_and_test(model, train_df, test_df, dataset_path, LABELS)\n",
    "\n",
    "\tprint(\"Run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_list = dir(NERArgs)\n",
    "\n",
    "import torch\n",
    "\n",
    "optimizer_state = torch.load(\"model/checkpoint-48000/training_args.bin\")\n",
    "\n",
    "attributes = list(dir(optimizer_state))\n",
    "\n",
    "\n",
    "# Find the intersection of the sets\n",
    "common_elements = list(set(attributes).intersection(set(ner_list)))\n",
    "\n",
    "print(common_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer_state.resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_state.warmup_ratio = 0.06\n",
    "optimizer_state.learning_rate = 1e-5\n",
    "optimizer_state.fp16 = True\n",
    "optimizer_state.logging_steps = 50\n",
    "#optimizer_state.n_gpu = 1\n",
    "optimizer_state.gradient_accumulation_steps = 1\n",
    "optimizer_state.output_dir = \"outputs/\"\n",
    "optimizer_state.num_train_epochs = 1\n",
    "optimizer_state.resume_from_checkpoint = True\n",
    "optimizer_state.ignore_data_skip = True\n",
    "\n",
    "\n",
    "# Save arguments with new attributes\n",
    "torch.save(optimizer_state, \"model/checkpoint-48000/training_args.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new file for results\n",
    "#with open(\"ner-results.txt\", \"w\") as file:\n",
    "#    file.write(\"Date\\tModel\\tRun\\tDataset\\tMicro F1\\tMacro F1\\tLabel Report\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the models if they work as expected\n",
    "# models: [\"xlm-r-large\", \"sloberta\", \"csebert\", \"xlm-r-base\", \"bertic\"]\n",
    "model = \"xlmrl-bcms-48\"\n",
    "run = \"test\"\n",
    "\n",
    "current_results_dict = train_and_test(model, train_df, test_df, dataset_path)\n",
    "\n",
    "# Add to the dict model name, dataset name and run\n",
    "current_results_dict[\"model\"] = model\n",
    "current_results_dict[\"run\"] = \"{}-{}\".format(model, run)\n",
    "current_results_dict[\"dataset\"] = dataset_path\n",
    "\n",
    "# Add to the file with results all important information\n",
    "#with open(\"ner-results-testing.txt\", \"a\") as file:\n",
    "#    file.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), current_results_dict[\"model\"], current_results_dict[\"run\"], current_results_dict[\"dataset\"], current_results_dict[\"micro F1\"], current_results_dict[\"macro F1\"], current_results_dict[\"Micro F1 Nikola\"], current_results_dict[\"Macro F1 Nikola\"], current_results_dict[\"label-report\"]))\n",
    "\n",
    "# Add to the original test_df y_preds\n",
    "#test_df[\"y_pred_{}_{}\".format(model, run)] = current_results_dict[\"y_pred\"]\n",
    "\n",
    "# Save entire dict just in case\n",
    "#with open(\"{}-{}-{}-backlog.json\".format(dataset_path,model,run), \"w\") as backlog:\n",
    "#    json.dump(current_results_dict, backlog, indent=2)\n",
    "\n",
    "print(\"Run {} finished.\".format(run))\n",
    "\n",
    "# At the end, save the test_df with all predictions\n",
    "#test_df.to_csv(\"{}-test_df-with-predictions.csv\".format(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Model</th>\n",
       "      <th>Run</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Label Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18/08/2023 16:39:46</td>\n",
       "      <td>xlm-r-large</td>\n",
       "      <td>xlm-r-large-0</td>\n",
       "      <td>datasets/hr500k.conllup_extracted.json</td>\n",
       "      <td>0.990291</td>\n",
       "      <td>0.918266</td>\n",
       "      <td>{'B-deriv-per': {'precision': 0.92105263157894...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18/08/2023 16:54:08</td>\n",
       "      <td>xlm-r-large</td>\n",
       "      <td>xlm-r-large-1</td>\n",
       "      <td>datasets/hr500k.conllup_extracted.json</td>\n",
       "      <td>0.990350</td>\n",
       "      <td>0.920143</td>\n",
       "      <td>{'B-deriv-per': {'precision': 0.92307692307692...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18/08/2023 17:02:56</td>\n",
       "      <td>sloberta</td>\n",
       "      <td>sloberta-0</td>\n",
       "      <td>datasets/hr500k.conllup_extracted.json</td>\n",
       "      <td>0.986618</td>\n",
       "      <td>0.889416</td>\n",
       "      <td>{'B-deriv-per': {'precision': 0.91666666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18/08/2023 17:11:46</td>\n",
       "      <td>sloberta</td>\n",
       "      <td>sloberta-1</td>\n",
       "      <td>datasets/hr500k.conllup_extracted.json</td>\n",
       "      <td>0.985681</td>\n",
       "      <td>0.884556</td>\n",
       "      <td>{'B-deriv-per': {'precision': 0.91666666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18/08/2023 17:20:49</td>\n",
       "      <td>csebert</td>\n",
       "      <td>csebert-0</td>\n",
       "      <td>datasets/hr500k.conllup_extracted.json</td>\n",
       "      <td>0.989861</td>\n",
       "      <td>0.913822</td>\n",
       "      <td>{'B-deriv-per': {'precision': 0.94444444444444...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>22/08/2023 12:17:27</td>\n",
       "      <td>xlmrb_bcms-96</td>\n",
       "      <td>xlmrb_bcms-96-1</td>\n",
       "      <td>datasets/hr500k.conllup_extracted.json</td>\n",
       "      <td>0.990096</td>\n",
       "      <td>0.920258</td>\n",
       "      <td>{'B-deriv-per': {'precision': 1.0, 'recall': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>22/08/2023 14:00:54</td>\n",
       "      <td>xlmrl_bcms-6</td>\n",
       "      <td>xlmrl_bcms-6-0</td>\n",
       "      <td>datasets/hr500k.conllup_extracted.json</td>\n",
       "      <td>0.989783</td>\n",
       "      <td>0.918359</td>\n",
       "      <td>{'B-deriv-per': {'precision': 0.97222222222222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>22/08/2023 14:15:39</td>\n",
       "      <td>xlmrl_bcms-6</td>\n",
       "      <td>xlmrl_bcms-6-1</td>\n",
       "      <td>datasets/hr500k.conllup_extracted.json</td>\n",
       "      <td>0.990604</td>\n",
       "      <td>0.923328</td>\n",
       "      <td>{'B-deriv-per': {'precision': 1.0, 'recall': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>22/08/2023 14:31:09</td>\n",
       "      <td>xlmrl_bcms-12</td>\n",
       "      <td>xlmrl_bcms-12-0</td>\n",
       "      <td>datasets/hr500k.conllup_extracted.json</td>\n",
       "      <td>0.990936</td>\n",
       "      <td>0.927423</td>\n",
       "      <td>{'B-deriv-per': {'precision': 0.97297297297297...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>22/08/2023 14:46:01</td>\n",
       "      <td>xlmrl_bcms-12</td>\n",
       "      <td>xlmrl_bcms-12-1</td>\n",
       "      <td>datasets/hr500k.conllup_extracted.json</td>\n",
       "      <td>0.990955</td>\n",
       "      <td>0.923719</td>\n",
       "      <td>{'B-deriv-per': {'precision': 0.94736842105263...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date          Model              Run  \\\n",
       "0   18/08/2023 16:39:46    xlm-r-large    xlm-r-large-0   \n",
       "1   18/08/2023 16:54:08    xlm-r-large    xlm-r-large-1   \n",
       "2   18/08/2023 17:02:56       sloberta       sloberta-0   \n",
       "3   18/08/2023 17:11:46       sloberta       sloberta-1   \n",
       "4   18/08/2023 17:20:49        csebert        csebert-0   \n",
       "..                  ...            ...              ...   \n",
       "75  22/08/2023 12:17:27  xlmrb_bcms-96  xlmrb_bcms-96-1   \n",
       "76  22/08/2023 14:00:54   xlmrl_bcms-6   xlmrl_bcms-6-0   \n",
       "77  22/08/2023 14:15:39   xlmrl_bcms-6   xlmrl_bcms-6-1   \n",
       "78  22/08/2023 14:31:09  xlmrl_bcms-12  xlmrl_bcms-12-0   \n",
       "79  22/08/2023 14:46:01  xlmrl_bcms-12  xlmrl_bcms-12-1   \n",
       "\n",
       "                                   Dataset  Micro F1  Macro F1  \\\n",
       "0   datasets/hr500k.conllup_extracted.json  0.990291  0.918266   \n",
       "1   datasets/hr500k.conllup_extracted.json  0.990350  0.920143   \n",
       "2   datasets/hr500k.conllup_extracted.json  0.986618  0.889416   \n",
       "3   datasets/hr500k.conllup_extracted.json  0.985681  0.884556   \n",
       "4   datasets/hr500k.conllup_extracted.json  0.989861  0.913822   \n",
       "..                                     ...       ...       ...   \n",
       "75  datasets/hr500k.conllup_extracted.json  0.990096  0.920258   \n",
       "76  datasets/hr500k.conllup_extracted.json  0.989783  0.918359   \n",
       "77  datasets/hr500k.conllup_extracted.json  0.990604  0.923328   \n",
       "78  datasets/hr500k.conllup_extracted.json  0.990936  0.927423   \n",
       "79  datasets/hr500k.conllup_extracted.json  0.990955  0.923719   \n",
       "\n",
       "                                         Label Report  \n",
       "0   {'B-deriv-per': {'precision': 0.92105263157894...  \n",
       "1   {'B-deriv-per': {'precision': 0.92307692307692...  \n",
       "2   {'B-deriv-per': {'precision': 0.91666666666666...  \n",
       "3   {'B-deriv-per': {'precision': 0.91666666666666...  \n",
       "4   {'B-deriv-per': {'precision': 0.94444444444444...  \n",
       "..                                                ...  \n",
       "75  {'B-deriv-per': {'precision': 1.0, 'recall': 0...  \n",
       "76  {'B-deriv-per': {'precision': 0.97222222222222...  \n",
       "77  {'B-deriv-per': {'precision': 1.0, 'recall': 0...  \n",
       "78  {'B-deriv-per': {'precision': 0.97297297297297...  \n",
       "79  {'B-deriv-per': {'precision': 0.94736842105263...  \n",
       "\n",
       "[80 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the txt with results\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.read_csv(\"ner-results.txt\", sep=\"\\t\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Model'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get average micro and macro F1\n",
    "dataset = \"datasets/hr500k.conllup_extracted.json\"\n",
    "\n",
    "# Define the dataset to inspect\n",
    "import matplotlib as plt\n",
    "results[results[\"Dataset\"] == dataset].groupby(\"Model\")[\"Micro F1\"].mean().plot(kind=\"line\", color=\"blue\")\n",
    "results[results[\"Dataset\"] == dataset].groupby(\"Model\")[\"Macro F1\"].mean().plot(kind=\"line\",color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dataset</th>\n",
       "      <th>Run</th>\n",
       "      <th>datasets/SUK.CoNLL-U/elexiswsd.ud.conllu_extracted.json</th>\n",
       "      <th>datasets/SUK.CoNLL-U/senticoref.ud.conllu_extracted.json</th>\n",
       "      <th>datasets/hr500k.conllup_extracted.json</th>\n",
       "      <th>datasets/reldi-normtagner-hr.conllup_extracted.json</th>\n",
       "      <th>datasets/reldi-normtagner-sr.conllup_extracted.json</th>\n",
       "      <th>datasets/set.sr.plus.conllup_extracted.json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bertic-0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bertic-1</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csebert-0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>csebert-1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sloberta-0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sloberta-1</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xlm-r-base-0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xlm-r-base-1</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xlm-r-large-0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xlm-r-large-1</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xlmrb_bcms-12-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xlmrb_bcms-12-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xlmrb_bcms-24-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xlmrb_bcms-24-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>xlmrb_bcms-36-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>xlmrb_bcms-36-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>xlmrb_bcms-48-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>xlmrb_bcms-48-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>xlmrb_bcms-60-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xlmrb_bcms-60-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xlmrb_bcms-72-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>xlmrb_bcms-72-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xlmrb_bcms-84-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>xlmrb_bcms-84-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>xlmrb_bcms-96-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>xlmrb_bcms-96-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>xlmrl_bcms-12-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>xlmrl_bcms-12-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>xlmrl_bcms-6-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>xlmrl_bcms-6-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Dataset              Run  \\\n",
       "0               bertic-0   \n",
       "1               bertic-1   \n",
       "2              csebert-0   \n",
       "3              csebert-1   \n",
       "4             sloberta-0   \n",
       "5             sloberta-1   \n",
       "6           xlm-r-base-0   \n",
       "7           xlm-r-base-1   \n",
       "8          xlm-r-large-0   \n",
       "9          xlm-r-large-1   \n",
       "10       xlmrb_bcms-12-0   \n",
       "11       xlmrb_bcms-12-1   \n",
       "12       xlmrb_bcms-24-0   \n",
       "13       xlmrb_bcms-24-1   \n",
       "14       xlmrb_bcms-36-0   \n",
       "15       xlmrb_bcms-36-1   \n",
       "16       xlmrb_bcms-48-0   \n",
       "17       xlmrb_bcms-48-1   \n",
       "18       xlmrb_bcms-60-0   \n",
       "19       xlmrb_bcms-60-1   \n",
       "20       xlmrb_bcms-72-0   \n",
       "21       xlmrb_bcms-72-1   \n",
       "22       xlmrb_bcms-84-0   \n",
       "23       xlmrb_bcms-84-1   \n",
       "24       xlmrb_bcms-96-0   \n",
       "25       xlmrb_bcms-96-1   \n",
       "26       xlmrl_bcms-12-0   \n",
       "27       xlmrl_bcms-12-1   \n",
       "28        xlmrl_bcms-6-0   \n",
       "29        xlmrl_bcms-6-1   \n",
       "\n",
       "Dataset  datasets/SUK.CoNLL-U/elexiswsd.ud.conllu_extracted.json  \\\n",
       "0                                                     0.44         \n",
       "1                                                     0.43         \n",
       "2                                                     0.79         \n",
       "3                                                     0.78         \n",
       "4                                                     0.36         \n",
       "5                                                     0.34         \n",
       "6                                                     0.69         \n",
       "7                                                     0.74         \n",
       "8                                                     0.82         \n",
       "9                                                     0.79         \n",
       "10                                                     NaN         \n",
       "11                                                     NaN         \n",
       "12                                                     NaN         \n",
       "13                                                     NaN         \n",
       "14                                                     NaN         \n",
       "15                                                     NaN         \n",
       "16                                                     NaN         \n",
       "17                                                     NaN         \n",
       "18                                                     NaN         \n",
       "19                                                     NaN         \n",
       "20                                                     NaN         \n",
       "21                                                     NaN         \n",
       "22                                                     NaN         \n",
       "23                                                     NaN         \n",
       "24                                                     NaN         \n",
       "25                                                     NaN         \n",
       "26                                                     NaN         \n",
       "27                                                     NaN         \n",
       "28                                                     NaN         \n",
       "29                                                     NaN         \n",
       "\n",
       "Dataset  datasets/SUK.CoNLL-U/senticoref.ud.conllu_extracted.json  \\\n",
       "0                                                     0.81          \n",
       "1                                                     0.82          \n",
       "2                                                     0.82          \n",
       "3                                                     0.83          \n",
       "4                                                     0.84          \n",
       "5                                                     0.83          \n",
       "6                                                     0.83          \n",
       "7                                                     0.82          \n",
       "8                                                     0.84          \n",
       "9                                                     0.84          \n",
       "10                                                     NaN          \n",
       "11                                                     NaN          \n",
       "12                                                     NaN          \n",
       "13                                                     NaN          \n",
       "14                                                     NaN          \n",
       "15                                                     NaN          \n",
       "16                                                     NaN          \n",
       "17                                                     NaN          \n",
       "18                                                     NaN          \n",
       "19                                                     NaN          \n",
       "20                                                     NaN          \n",
       "21                                                     NaN          \n",
       "22                                                     NaN          \n",
       "23                                                     NaN          \n",
       "24                                                     NaN          \n",
       "25                                                     NaN          \n",
       "26                                                     NaN          \n",
       "27                                                     NaN          \n",
       "28                                                     NaN          \n",
       "29                                                     NaN          \n",
       "\n",
       "Dataset  datasets/hr500k.conllup_extracted.json  \\\n",
       "0                                          0.92   \n",
       "1                                          0.92   \n",
       "2                                          0.91   \n",
       "3                                          0.91   \n",
       "4                                          0.89   \n",
       "5                                          0.88   \n",
       "6                                          0.91   \n",
       "7                                          0.90   \n",
       "8                                          0.92   \n",
       "9                                          0.92   \n",
       "10                                         0.91   \n",
       "11                                         0.91   \n",
       "12                                         0.92   \n",
       "13                                         0.92   \n",
       "14                                         0.92   \n",
       "15                                         0.92   \n",
       "16                                         0.92   \n",
       "17                                         0.91   \n",
       "18                                         0.92   \n",
       "19                                         0.92   \n",
       "20                                         0.92   \n",
       "21                                         0.92   \n",
       "22                                         0.92   \n",
       "23                                         0.92   \n",
       "24                                         0.92   \n",
       "25                                         0.92   \n",
       "26                                         0.93   \n",
       "27                                         0.92   \n",
       "28                                         0.92   \n",
       "29                                         0.92   \n",
       "\n",
       "Dataset  datasets/reldi-normtagner-hr.conllup_extracted.json  \\\n",
       "0                                                     0.62     \n",
       "1                                                     0.62     \n",
       "2                                                     0.79     \n",
       "3                                                     0.79     \n",
       "4                                                     0.50     \n",
       "5                                                     0.55     \n",
       "6                                                     0.72     \n",
       "7                                                     0.70     \n",
       "8                                                     0.73     \n",
       "9                                                     0.75     \n",
       "10                                                     NaN     \n",
       "11                                                     NaN     \n",
       "12                                                     NaN     \n",
       "13                                                     NaN     \n",
       "14                                                     NaN     \n",
       "15                                                     NaN     \n",
       "16                                                     NaN     \n",
       "17                                                     NaN     \n",
       "18                                                     NaN     \n",
       "19                                                     NaN     \n",
       "20                                                     NaN     \n",
       "21                                                     NaN     \n",
       "22                                                     NaN     \n",
       "23                                                     NaN     \n",
       "24                                                     NaN     \n",
       "25                                                     NaN     \n",
       "26                                                     NaN     \n",
       "27                                                     NaN     \n",
       "28                                                     NaN     \n",
       "29                                                     NaN     \n",
       "\n",
       "Dataset  datasets/reldi-normtagner-sr.conllup_extracted.json  \\\n",
       "0                                                     0.51     \n",
       "1                                                     0.48     \n",
       "2                                                     0.67     \n",
       "3                                                     0.66     \n",
       "4                                                     0.44     \n",
       "5                                                     0.44     \n",
       "6                                                     0.57     \n",
       "7                                                     0.61     \n",
       "8                                                     0.72     \n",
       "9                                                     0.67     \n",
       "10                                                     NaN     \n",
       "11                                                     NaN     \n",
       "12                                                     NaN     \n",
       "13                                                     NaN     \n",
       "14                                                     NaN     \n",
       "15                                                     NaN     \n",
       "16                                                     NaN     \n",
       "17                                                     NaN     \n",
       "18                                                     NaN     \n",
       "19                                                     NaN     \n",
       "20                                                     NaN     \n",
       "21                                                     NaN     \n",
       "22                                                     NaN     \n",
       "23                                                     NaN     \n",
       "24                                                     NaN     \n",
       "25                                                     NaN     \n",
       "26                                                     NaN     \n",
       "27                                                     NaN     \n",
       "28                                                     NaN     \n",
       "29                                                     NaN     \n",
       "\n",
       "Dataset  datasets/set.sr.plus.conllup_extracted.json  \n",
       "0                                               0.79  \n",
       "1                                               0.79  \n",
       "2                                               0.91  \n",
       "3                                               0.90  \n",
       "4                                               0.77  \n",
       "5                                               0.71  \n",
       "6                                               0.89  \n",
       "7                                               0.86  \n",
       "8                                               0.93  \n",
       "9                                               0.93  \n",
       "10                                               NaN  \n",
       "11                                               NaN  \n",
       "12                                               NaN  \n",
       "13                                               NaN  \n",
       "14                                               NaN  \n",
       "15                                               NaN  \n",
       "16                                               NaN  \n",
       "17                                               NaN  \n",
       "18                                               NaN  \n",
       "19                                               NaN  \n",
       "20                                               NaN  \n",
       "21                                               NaN  \n",
       "22                                               NaN  \n",
       "23                                               NaN  \n",
       "24                                               NaN  \n",
       "25                                               NaN  \n",
       "26                                               NaN  \n",
       "27                                               NaN  \n",
       "28                                               NaN  \n",
       "29                                               NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results[\"Dataset\"] == dataset].groupby(\"Model\")[\"Macro F1\"].mean().round(2)\n",
    "\n",
    "results[\"Macro F1\"] = results[\"Macro F1\"].round(2)\n",
    "\n",
    "# Pivot the DataFrame to rearrange columns into rows\n",
    "pivot_df = results.pivot(index='Run', columns='Dataset', values='Macro F1')\n",
    "\n",
    "# Reset the index to have 'Model' as a column\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame to rearrange columns into rows\n",
    "pivot_df.to_csv(\"ner-results-summary-table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the df with all the predictions\n",
    "import numpy as np\n",
    "\n",
    "pred_df = pd.read_csv(\"datasets/hr500k.conllup_extracted.json-test_df-with-predictions.csv\", index_col = 0)\n",
    "\n",
    "# Analyze instances where models are wrong\n",
    "pred_df[\"match\"] = np.where(pred_df[\"labels\"] != pred_df[\"y_pred_xlm-r-large_0\"], \"no\", \"yes\")\n",
    "pred_df.match.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[pred_df[\"match\"] == \"no\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
