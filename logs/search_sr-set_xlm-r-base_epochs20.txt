wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:483: UserWarning: The eval_df parameter has been renamed to eval_data. Using eval_df will raise an error in a future version.
  warnings.warn(
INFO:simpletransformers.ner.ner_model: Converting to features started.
['O', 'B-loc', 'B-org', 'B-per', 'I-per', 'B-deriv-per', 'I-org', 'I-loc', 'B-misc', 'I-misc']
(74259, 3) (11421, 3) (11993, 3)
     sentence_id      words labels
726            0      Kazna      O
727            0  medijskom      O
728            0     mogulu      O
729            0   obnovila      O
730            0     debatu      O
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.15.8
wandb: Run data is saved locally in /home/tajak/NER-recognition/wandb/run-20230823_123352-q3153owb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-blaze-108
wandb: ⭐️ View project at https://wandb.ai/tajak/NER
wandb: 🚀 View run at https://wandb.ai/tajak/NER/runs/q3153owb
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Converting to features started.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.31449743249078294, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.1021970712176677, 'precision': 0.7806004618937644, 'recall': 0.8028503562945368, 'f1_score': 0.791569086651054}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06139255522875421, 'precision': 0.8415051311288484, 'recall': 0.8764845605700713, 'f1_score': 0.8586387434554974}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05161278133294476, 'precision': 0.8739789964994166, 'recall': 0.8895486935866983, 'f1_score': 0.8816951147733961}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05552184838304686, 'precision': 0.8942420681551116, 'recall': 0.9038004750593824, 'f1_score': 0.8989958653278205}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.050573312574059275, 'precision': 0.8995327102803738, 'recall': 0.9144893111638955, 'f1_score': 0.9069493521790342}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.055607416067157744, 'precision': 0.9054842473745625, 'recall': 0.9216152019002375, 'f1_score': 0.9134785167745733}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05601642248715941, 'precision': 0.899188876013905, 'recall': 0.9216152019002375, 'f1_score': 0.9102639296187683}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.055010177370998316, 'precision': 0.9043173862310385, 'recall': 0.9204275534441805, 'f1_score': 0.9123013537374927}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.054809904953338026, 'precision': 0.9085580304806565, 'recall': 0.9204275534441805, 'f1_score': 0.9144542772861357}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05988137846953347, 'precision': 0.9118683901292597, 'recall': 0.9216152019002375, 'f1_score': 0.9167158889545186}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.059564568473325016, 'precision': 0.9098360655737705, 'recall': 0.9228028503562945, 'f1_score': 0.9162735849056604}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05913302125478747, 'precision': 0.9096244131455399, 'recall': 0.9204275534441805, 'f1_score': 0.9149940968122786}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05900083809390355, 'precision': 0.9186320754716981, 'recall': 0.9251781472684085, 'f1_score': 0.9218934911242602}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.059078742724468884, 'precision': 0.9086651053864169, 'recall': 0.9216152019002375, 'f1_score': 0.9150943396226414}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05934519258935053, 'precision': 0.9187279151943463, 'recall': 0.9263657957244655, 'f1_score': 0.9225310467179184}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06069910453505199, 'precision': 0.9079254079254079, 'recall': 0.9251781472684085, 'f1_score': 0.9164705882352941}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06089426427000157, 'precision': 0.9144196951934349, 'recall': 0.9263657957244655, 'f1_score': 0.9203539823008849}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06085014926424292, 'precision': 0.9131455399061033, 'recall': 0.9239904988123515, 'f1_score': 0.9185360094451003}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06108497824389991, 'precision': 0.9110070257611241, 'recall': 0.9239904988123515, 'f1_score': 0.9174528301886792}
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Training loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     eval_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      f1_score ▁▇██████████████████
wandb:   global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            lr ▄▇███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁
wandb:     precision ▁▇▇█████████████████
wandb:        recall ▁▇██████████████████
wandb:    train_loss █▄▃▂▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: Training loss 0.00293
wandb:     eval_loss 0.06108
wandb:      f1_score 0.91745
wandb:   global_step 2080
wandb:            lr 0.0
wandb:     precision 0.91101
wandb:        recall 0.92399
wandb:    train_loss 0.0018
wandb: 
wandb: 🚀 View run earnest-blaze-108 at: https://wandb.ai/tajak/NER/runs/q3153owb
wandb: ️⚡ View job at https://wandb.ai/tajak/NER/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkwODMzNTg1/version_details/v7
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230823_123352-q3153owb/logs
