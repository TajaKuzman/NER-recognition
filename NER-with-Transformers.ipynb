{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER using Transformer models and special tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset used is a dictionary with the following lists:\n",
    "- tokens (list of words for each sentence): [\"This\", \"is\", \"an\", \"example\", \".\"]\n",
    "- ner_tags (list of NER tags for each word): ['O', \"O\", 'O', 'O', 'O']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenizers separate words to subword level. To make the model work on world level, we need to create our own tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=7\n"
     ]
    }
   ],
   "source": [
    "# Define the gpu on the gpu machine\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.28.0\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "# use the seqeval package which has measures for evaluation sequence classification\n",
    "#!pip install seqeval\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-loc', 'B-org', 'B-per', 'I-per', 'B-deriv-per', 'I-org', 'I-loc', 'B-misc', 'I-misc', 'I-deriv-per']\n",
      "(398681, 3) (51190, 3) (49764, 3)\n",
      "    sentence_id      words labels\n",
      "717  set.hr-s36      Kazna      O\n",
      "718  set.hr-s36  medijskom      O\n",
      "719  set.hr-s36     mogulu      O\n",
      "720  set.hr-s36   obnovila      O\n",
      "721  set.hr-s36   raspravu      O\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset\n",
    "\n",
    "# Code for python script\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"dataset\", help=\"path to the dataset in JSON format\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = args.dataset\n",
    "\"\"\"\n",
    "# Define the path to the dataset\n",
    "dataset_path = \"datasets/hr500k.conllup_extracted.json\"\n",
    "\n",
    "# Load the json file\n",
    "with open(dataset_path, \"r\") as file:\n",
    "    json_dict = json.load(file)\n",
    "\n",
    "# Open the train, eval and test dictionaries as DataFrames\n",
    "train_df = pd.DataFrame(json_dict[\"train\"])\n",
    "test_df = pd.DataFrame(json_dict[\"test\"])\n",
    "dev_df = pd.DataFrame(json_dict[\"dev\"])\n",
    "\n",
    "# Define the labels\n",
    "LABELS = json_dict[\"labels\"]\n",
    "print(LABELS)\n",
    "\n",
    "print(train_df.shape, test_df.shape, dev_df.shape)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use torch and transformers, we need to transform the dataset into a specific format. We need to create two lists: token_docs which is a list of lists of token strings, and token_tags which is a list of lists of tag strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>Kazna</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>medijskom</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>mogulu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>obnovila</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>raspravu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence_id      words labels\n",
       "717  set.hr-s36      Kazna      O\n",
       "718  set.hr-s36  medijskom      O\n",
       "719  set.hr-s36     mogulu      O\n",
       "720  set.hr-s36   obnovila      O\n",
       "721  set.hr-s36   raspravu      O"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-loc', 2: 'B-org', 3: 'B-per', 4: 'I-per', 5: 'B-deriv-per', 6: 'I-org', 7: 'I-loc', 8: 'B-misc', 9: 'I-misc', 10: 'I-deriv-per'}\n",
      "{'O': 0, 'B-loc': 1, 'B-org': 2, 'B-per': 3, 'I-per': 4, 'B-deriv-per': 5, 'I-org': 6, 'I-loc': 7, 'B-misc': 8, 'I-misc': 9, 'I-deriv-per': 10}\n"
     ]
    }
   ],
   "source": [
    "# Define encodings for the NER tags - get it from the dataset (labels), e.g., tags = [\"O\", \"B-PER\", \"I-PER\"]\n",
    "\n",
    "index2tag = {idx: tag for idx, tag in enumerate(LABELS)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(LABELS)}\n",
    "\n",
    "print(index2tag, tag2index, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>Kazna</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>medijskom</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>mogulu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>obnovila</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>raspravu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499630</th>\n",
       "      <td>prosir-s120</td>\n",
       "      <td>nećemo</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499631</th>\n",
       "      <td>prosir-s120</td>\n",
       "      <td>tako</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499632</th>\n",
       "      <td>prosir-s120</td>\n",
       "      <td>skoro</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499633</th>\n",
       "      <td>prosir-s120</td>\n",
       "      <td>zaboraviti</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499634</th>\n",
       "      <td>prosir-s120</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398681 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id       words labels\n",
       "717      set.hr-s36       Kazna      O\n",
       "718      set.hr-s36   medijskom      O\n",
       "719      set.hr-s36      mogulu      O\n",
       "720      set.hr-s36    obnovila      O\n",
       "721      set.hr-s36    raspravu      O\n",
       "...             ...         ...    ...\n",
       "499630  prosir-s120      nećemo      O\n",
       "499631  prosir-s120        tako      O\n",
       "499632  prosir-s120       skoro      O\n",
       "499633  prosir-s120  zaboraviti      O\n",
       "499634  prosir-s120           .      O\n",
       "\n",
       "[398681 rows x 3 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>Kazna</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>medijskom</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>mogulu</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>obnovila</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>raspravu</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>u</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>set.hr-s36</td>\n",
       "      <td>Makedoniji</td>\n",
       "      <td>B-loc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>Neki</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>tvrde</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>da</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>je</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>presuda</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>Veliji</td>\n",
       "      <td>B-per</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>Ramkovskom</td>\n",
       "      <td>I-per</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>napad</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>na</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>slobodu</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>medija</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>set.hr-s37</td>\n",
       "      <td>dok</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence_id       words labels  labels_index\n",
       "717  set.hr-s36       Kazna      O             0\n",
       "718  set.hr-s36   medijskom      O             0\n",
       "719  set.hr-s36      mogulu      O             0\n",
       "720  set.hr-s36    obnovila      O             0\n",
       "721  set.hr-s36    raspravu      O             0\n",
       "722  set.hr-s36           u      O             0\n",
       "723  set.hr-s36  Makedoniji  B-loc             1\n",
       "724  set.hr-s37        Neki      O             0\n",
       "725  set.hr-s37       tvrde      O             0\n",
       "726  set.hr-s37          da      O             0\n",
       "727  set.hr-s37          je      O             0\n",
       "728  set.hr-s37     presuda      O             0\n",
       "729  set.hr-s37      Veliji  B-per             3\n",
       "730  set.hr-s37  Ramkovskom  I-per             4\n",
       "731  set.hr-s37       napad      O             0\n",
       "732  set.hr-s37          na      O             0\n",
       "733  set.hr-s37     slobodu      O             0\n",
       "734  set.hr-s37      medija      O             0\n",
       "735  set.hr-s37           ,      O             0\n",
       "736  set.hr-s37         dok      O             0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map numbers for tags to the dataframe\n",
    "train_df[\"labels_index\"] = [tag2index[x] for x in train_df.labels]\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>set.hr-s1</td>\n",
       "      <td>Proces</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>set.hr-s1</td>\n",
       "      <td>privatizacije</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>set.hr-s1</td>\n",
       "      <td>na</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>set.hr-s1</td>\n",
       "      <td>Kosovu</td>\n",
       "      <td>B-loc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>set.hr-s1</td>\n",
       "      <td>pod</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>set.hr-s1</td>\n",
       "      <td>povećalom</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>set.hr-s2</td>\n",
       "      <td>Kosovo</td>\n",
       "      <td>B-loc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>set.hr-s2</td>\n",
       "      <td>ozbiljno</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>set.hr-s2</td>\n",
       "      <td>analizira</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>set.hr-s2</td>\n",
       "      <td>proces</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>set.hr-s2</td>\n",
       "      <td>privatizacije</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>set.hr-s2</td>\n",
       "      <td>u</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>set.hr-s2</td>\n",
       "      <td>svjetlu</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>set.hr-s2</td>\n",
       "      <td>učestalih</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>set.hr-s2</td>\n",
       "      <td>pritužbi</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>set.hr-s2</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>set.hr-s3</td>\n",
       "      <td>Feronikel</td>\n",
       "      <td>B-org</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>set.hr-s3</td>\n",
       "      <td>je</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>set.hr-s3</td>\n",
       "      <td>privatiziran</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>set.hr-s3</td>\n",
       "      <td>prije</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id          words labels  labels_index\n",
       "0    set.hr-s1         Proces      O             0\n",
       "1    set.hr-s1  privatizacije      O             0\n",
       "2    set.hr-s1             na      O             0\n",
       "3    set.hr-s1         Kosovu  B-loc             1\n",
       "4    set.hr-s1            pod      O             0\n",
       "5    set.hr-s1      povećalom      O             0\n",
       "6    set.hr-s2         Kosovo  B-loc             1\n",
       "7    set.hr-s2       ozbiljno      O             0\n",
       "8    set.hr-s2      analizira      O             0\n",
       "9    set.hr-s2         proces      O             0\n",
       "10   set.hr-s2  privatizacije      O             0\n",
       "11   set.hr-s2              u      O             0\n",
       "12   set.hr-s2        svjetlu      O             0\n",
       "13   set.hr-s2      učestalih      O             0\n",
       "14   set.hr-s2       pritužbi      O             0\n",
       "15   set.hr-s2              .      O             0\n",
       "16   set.hr-s3      Feronikel  B-org             2\n",
       "17   set.hr-s3             je      O             0\n",
       "18   set.hr-s3   privatiziran      O             0\n",
       "19   set.hr-s3          prije      O             0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map numbers for tags to the dataframe\n",
    "dev_df[\"labels_index\"] = [tag2index[x] for x in dev_df.labels]\n",
    "dev_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataset to get the format we need\n",
    "\n",
    "def read_dataframe(df):\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "\n",
    "    for i in df.sentence_id.unique():\n",
    "        subset = df[df[\"sentence_id\"] == i]\n",
    "        current_word_list = subset.words.to_list()\n",
    "        current_label_list = subset.labels_index.to_list()\n",
    "        token_docs.append(current_word_list)\n",
    "        tag_docs.append(current_label_list)\n",
    "\n",
    "    return token_docs, tag_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_tags = read_dataframe(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for eval and test\n",
    "eval_texts, eval_tags = read_dataframe(dev_df)\n",
    "#test_texts, test_tags = read_dataframe(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kazna', 'medijskom', 'mogulu', 'obnovila', 'raspravu', 'u', 'Makedoniji']\n",
      "[0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0], train_tags[0], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Slovenian datasets: create train, test, val split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model which serves as the foundation for tokenization and fine-tuning\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n",
    "\n",
    "# Define our own XLM-R-based tokenizer\n",
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "                                     hidden_states=outputs.hidden_states, attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to connect indices for tags with tag names\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, num_labels=len(LABELS), id2label=index2tag, label2id=tag2index)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xlmr_model = (XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create new id tags for each word - we iterate through the words\n",
    "# and if it isn't a new word, we assign the IGN tag (ignore) to it\n",
    "\n",
    "def tokenize_and_align_labels(texts, tags):\n",
    "    encodings = xlmr_tokenizer(texts, truncation=True, is_split_into_words=True)\n",
    "    encoded_labels = []\n",
    "    for idx, label in enumerate(tags):\n",
    "        word_ids = encodings.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100) # new IGN tag!\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        encoded_labels.append(label_ids)\n",
    "    return encodings, encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings, train_encoded_labels = tokenize_and_align_labels(train_texts, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat with eval set\n",
    "eval_encodings, eval_encoded_labels = tokenize_and_align_labels(eval_texts, eval_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to create a torch dataset object\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(train_encodings, train_encoded_labels)\n",
    "eval_dataset = NERDataset(eval_encodings, eval_encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}\n",
    "\n",
    "def model_init():\n",
    "    return (XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2475' max='2475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2475/2475 04:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.047518</td>\n",
       "      <td>0.838362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.040806</td>\n",
       "      <td>0.870326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.881725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForTokenClassification, Trainer\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "logging_steps = len(train_texts) // batch_size\n",
    "model_name = f\"{xlmr_model_name}-finetuned-test\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                 log_level=\"error\",\n",
    "                                 num_train_epochs=num_epochs,\n",
    "                                 per_device_train_batch_size=batch_size,\n",
    "                                 per_device_eval_batch_size=batch_size,\n",
    "                                 evaluation_strategy=\"epoch\",\n",
    "                                 save_steps=1e6,\n",
    "                                 weight_decay=0.01,\n",
    "                                 disable_tqdm=False,\n",
    "                                 logging_steps=logging_steps,\n",
    "                                 push_to_hub=False\n",
    "                                )\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=eval_dataset,\n",
    "                  tokenizer=xlmr_tokenizer)\n",
    "trainer.train()\n",
    "\n",
    "finetuned_model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     y_pred, y_true \u001b[39m=\u001b[39m align_predictions(preds, input_ids)\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m {}\n\u001b[0;32m---> 13\u001b[0m compute_metrics_test(text)\n",
      "Cell \u001b[0;32mIn[119], line 10\u001b[0m, in \u001b[0;36mcompute_metrics_test\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m preds \u001b[39m=\u001b[39m [LABELS[p] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m predictions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()]\n\u001b[0;32m---> 10\u001b[0m y_pred, y_true \u001b[39m=\u001b[39m align_predictions(preds, input_ids)\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m {}\n",
      "Cell \u001b[0;32mIn[104], line 2\u001b[0m, in \u001b[0;36malign_predictions\u001b[0;34m(predictions, label_ids)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39malign_predictions\u001b[39m(predictions, label_ids):\n\u001b[0;32m----> 2\u001b[0m     preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(predictions, axis\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m     batch_size, seq_len \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m     labels_list, preds_list \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1242\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1242\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/NER-recognition/ner/lib/python3.8/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# Let's test the model\n",
    "text = \"Ime mi je Taja.\"\n",
    "\n",
    "def compute_metrics_test(text):\n",
    "    tokens = xlmr_tokenizer(text).tokens()\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs  =  finetuned_model(input_ids)\n",
    "    predictions = torch.argmax(outputs[0], dim=2)\n",
    "    preds = [LABELS[p] for p in predictions[0].cpu().numpy()]\n",
    "    y_pred, y_true = align_predictions(preds, input_ids)\n",
    "    return {}\n",
    "\n",
    "compute_metrics_test(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jeff</td>\n",
       "      <td>▁De</td>\n",
       "      <td>an</td>\n",
       "      <td>▁ist</td>\n",
       "      <td>▁ein</td>\n",
       "      <td>▁Informati</td>\n",
       "      <td>ker</td>\n",
       "      <td>▁bei</td>\n",
       "      <td>▁Google</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁Kaliforni</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-per</td>\n",
       "      <td>I-per</td>\n",
       "      <td>I-per</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-org</td>\n",
       "      <td>O</td>\n",
       "      <td>B-loc</td>\n",
       "      <td>I-loc</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3     4     5           6    7     8        9   \\\n",
       "Tokens  <s>  ▁Jeff    ▁De     an  ▁ist  ▁ein  ▁Informati  ker  ▁bei  ▁Google   \n",
       "Tags      O  B-per  I-per  I-per     O     O           O    O     O    B-org   \n",
       "\n",
       "         10          11     12    13  \n",
       "Tokens  ▁in  ▁Kaliforni     en  </s>  \n",
       "Tags      O       B-loc  I-loc     O  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test the model\n",
    "def tag_text(text, model, tokenizer):\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model(input_ids)\n",
    "    predictions = torch.argmax(outputs[0], dim=2)\n",
    "    preds = [LABELS[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
    "\n",
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "tag_text(text_de, finetuned_model, xlmr_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
