wandb: Currently logged in as: tajak. Use `wandb login --relogin` to force relogin
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:483: UserWarning: The eval_df parameter has been renamed to eval_data. Using eval_df will raise an error in a future version.
  warnings.warn(
INFO:simpletransformers.ner.ner_model: Converting to features started.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.8
wandb: Run data is saved locally in /home/tajak/NER-recognition/wandb/run-20230907_094003-88theicn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-salad-121
wandb: ⭐️ View project at https://wandb.ai/tajak/NER
wandb: 🚀 View run at https://wandb.ai/tajak/NER/runs/88theicn
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.047458726147942505, 'precision': 0.8513341804320204, 'recall': 0.883128295254833, 'f1_score': 0.8669398317877938}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.051737830700810214, 'precision': 0.8457943925233645, 'recall': 0.874780316344464, 'f1_score': 0.8600431965442764}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.055246789859136704, 'precision': 0.8572050500653026, 'recall': 0.8651142355008787, 'f1_score': 0.861141482615351}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.050137303392525416, 'precision': 0.8824057450628366, 'recall': 0.8637961335676626, 'f1_score': 0.8730017761989344}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.0650349664823225, 'precision': 0.8710089399744572, 'recall': 0.8989455184534271, 'f1_score': 0.8847567567567567}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05411969383160698, 'precision': 0.8729139922978177, 'recall': 0.8963093145869947, 'f1_score': 0.8844569694342077}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06057151858960564, 'precision': 0.8898450946643718, 'recall': 0.9086115992970123, 'f1_score': 0.8991304347826086}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.07324077980219434, 'precision': 0.8852672750977836, 'recall': 0.8949912126537786, 'f1_score': 0.8901026873497925}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.07261154386961238, 'precision': 0.8844005156854319, 'recall': 0.9042179261862917, 'f1_score': 0.8941994351509883}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.0717788986596365, 'precision': 0.8835117773019272, 'recall': 0.906414762741652, 'f1_score': 0.8948167425721102}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.07251940284045062, 'precision': 0.8891774891774892, 'recall': 0.9024604569420035, 'f1_score': 0.8957697339729612}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.07779225048433716, 'precision': 0.892468437091859, 'recall': 0.9007029876977153, 'f1_score': 0.896566805160726}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.07973105721499217, 'precision': 0.8954191875540191, 'recall': 0.9103690685413005, 'f1_score': 0.9028322440087145}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.07967247993324371, 'precision': 0.8981723237597912, 'recall': 0.9068541300527241, 'f1_score': 0.9024923480542195}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.08146536721601152, 'precision': 0.8961772371850565, 'recall': 0.906414762741652, 'f1_score': 0.9012669287898646}
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Training loss █▅▂▂▅▃▁▂▂▂▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁
wandb:     eval_loss ▁▂▃▂▅▂▄▆▆▆▆▇███
wandb:      f1_score ▂▁▁▃▅▅▇▆▇▇▇▇███
wandb:   global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            lr ▂▅▇███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:     precision ▂▁▃▆▄▅▇▆▆▆▇▇███
wandb:        recall ▄▃▁▁▆▆█▆▇▇▇▇█▇▇
wandb:    train_loss ▆▃▂█▃▃▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: Training loss 2e-05
wandb:     eval_loss 0.08147
wandb:      f1_score 0.90127
wandb:   global_step 9285
wandb:            lr 0.0
wandb:     precision 0.89618
wandb:        recall 0.90641
wandb:    train_loss 1e-05
wandb: 
wandb: 🚀 View run clear-salad-121 at: https://wandb.ai/tajak/NER/runs/88theicn
wandb: ️⚡ View job at https://wandb.ai/tajak/NER/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkwODMzNTg1/version_details/v17
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230907_094003-88theicn/logs
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:483: UserWarning: The eval_df parameter has been renamed to eval_data. Using eval_df will raise an error in a future version.
  warnings.warn(
INFO:simpletransformers.ner.ner_model: Converting to features started.
wandb: wandb version 0.15.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.8
wandb: Run data is saved locally in /home/tajak/NER-recognition/wandb/run-20230907_102555-by94yiuz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-pond-122
wandb: ⭐️ View project at https://wandb.ai/tajak/NER
wandb: 🚀 View run at https://wandb.ai/tajak/NER/runs/by94yiuz
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.12423323589842766, 'precision': 0.7281879194630873, 'recall': 0.7905282331511839, 'f1_score': 0.7580786026200873}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.09200767335947603, 'precision': 0.8354661791590493, 'recall': 0.8324225865209471, 'f1_score': 0.8339416058394161}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.09070007110101869, 'precision': 0.8311688311688312, 'recall': 0.8160291438979964, 'f1_score': 0.8235294117647061}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.10598713952727849, 'precision': 0.8310104529616724, 'recall': 0.8688524590163934, 'f1_score': 0.8495102404274265}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.12112347066824442, 'precision': 0.8566243194192378, 'recall': 0.8597449908925319, 'f1_score': 0.8581818181818182}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.12460422134507099, 'precision': 0.8656987295825771, 'recall': 0.8688524590163934, 'f1_score': 0.8672727272727273}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.14718099364004955, 'precision': 0.8858800773694391, 'recall': 0.8342440801457195, 'f1_score': 0.8592870544090057}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.1424849502557663, 'precision': 0.8202443280977313, 'recall': 0.8561020036429873, 'f1_score': 0.8377896613190731}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.14425346723582153, 'precision': 0.8639705882352942, 'recall': 0.8561020036429873, 'f1_score': 0.8600182982616652}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.16226839865206785, 'precision': 0.8722222222222222, 'recall': 0.8579234972677595, 'f1_score': 0.8650137741046832}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.17121864138450063, 'precision': 0.8795620437956204, 'recall': 0.8779599271402551, 'f1_score': 0.878760255241568}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.1788571155216141, 'precision': 0.8756855575868373, 'recall': 0.8724954462659381, 'f1_score': 0.874087591240876}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.17240420698210981, 'precision': 0.8858195211786372, 'recall': 0.8761384335154827, 'f1_score': 0.8809523809523808}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.18121824507897372, 'precision': 0.8864059590316573, 'recall': 0.8670309653916212, 'f1_score': 0.8766114180478821}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.1792400158678106, 'precision': 0.8880597014925373, 'recall': 0.8670309653916212, 'f1_score': 0.8774193548387097}
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Training loss █▃▃▃▄▅▂▂▂▂▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     eval_loss ▄▁▁▂▃▄▅▅▅▇▇█▇██
wandb:      f1_score ▁▅▅▆▇▇▇▆▇▇█████
wandb:   global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:            lr ▃▅███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:     precision ▁▆▆▆▇▇█▅▇▇█▇███
wandb:        recall ▁▄▃▇▇▇▅▆▆▆███▇▇
wandb:    train_loss ▅█▅▁▁▁▁▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: Training loss 0.0001
wandb:     eval_loss 0.17924
wandb:      f1_score 0.87742
wandb:   global_step 3000
wandb:            lr 0.0
wandb:     precision 0.88806
wandb:        recall 0.86703
wandb:    train_loss 0.0001
wandb: 
wandb: 🚀 View run zesty-pond-122 at: https://wandb.ai/tajak/NER/runs/by94yiuz
wandb: ️⚡ View job at https://wandb.ai/tajak/NER/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkwODMzNTg1/version_details/v17
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230907_102555-by94yiuz/logs
wandb: Currently logged in as: tajak. Use `wandb login --relogin` to force relogin
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:483: UserWarning: The eval_df parameter has been renamed to eval_data. Using eval_df will raise an error in a future version.
  warnings.warn(
INFO:simpletransformers.ner.ner_model: Converting to features started.
wandb: wandb version 0.15.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.8
wandb: Run data is saved locally in /home/tajak/NER-recognition/wandb/run-20230907_104235-r9t51vgg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-disco-123
wandb: ⭐️ View project at https://wandb.ai/tajak/NER
wandb: 🚀 View run at https://wandb.ai/tajak/NER/runs/r9t51vgg
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06217524694077281, 'precision': 0.8536866359447005, 'recall': 0.8800475059382423, 'f1_score': 0.8666666666666667}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.04890044308211064, 'precision': 0.9018691588785047, 'recall': 0.9168646080760094, 'f1_score': 0.9093050647820966}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05146279857179442, 'precision': 0.9204275534441805, 'recall': 0.9204275534441805, 'f1_score': 0.9204275534441805}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05277897192935421, 'precision': 0.910271546635183, 'recall': 0.9156769596199525, 'f1_score': 0.9129662522202487}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05131932712739304, 'precision': 0.9121779859484778, 'recall': 0.9251781472684085, 'f1_score': 0.9186320754716982}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06412229771865272, 'precision': 0.9089834515366431, 'recall': 0.9133016627078385, 'f1_score': 0.9111374407582938}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.0635293232153073, 'precision': 0.9068396226415094, 'recall': 0.9133016627078385, 'f1_score': 0.9100591715976332}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.0652305452951433, 'precision': 0.9098360655737705, 'recall': 0.9228028503562945, 'f1_score': 0.9162735849056604}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06982046638405784, 'precision': 0.9198564593301436, 'recall': 0.9133016627078385, 'f1_score': 0.9165673420738976}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05495096508182325, 'precision': 0.9119718309859155, 'recall': 0.9228028503562945, 'f1_score': 0.9173553719008264}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06123082315870401, 'precision': 0.9155920281359906, 'recall': 0.9275534441805225, 'f1_score': 0.9215339233038348}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06665593679957653, 'precision': 0.91725768321513, 'recall': 0.9216152019002375, 'f1_score': 0.919431279620853}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06639206355906148, 'precision': 0.9243498817966903, 'recall': 0.9287410926365796, 'f1_score': 0.9265402843601895}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06560153389283363, 'precision': 0.917550058892815, 'recall': 0.9251781472684085, 'f1_score': 0.9213483146067415}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06604819708552474, 'precision': 0.9152941176470588, 'recall': 0.9239904988123515, 'f1_score': 0.9196217494089834}
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Training loss █▄▂▃▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb:     eval_loss ▅▁▂▂▂▆▆▆█▃▅▇▇▇▇
wandb:      f1_score ▁▆▇▆▇▆▆▇▇▇▇▇█▇▇
wandb:   global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:            lr ▅███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:     precision ▁▆█▇▇▆▆▇█▇▇▇█▇▇
wandb:        recall ▁▆▇▆▇▆▆▇▆▇█▇█▇▇
wandb:    train_loss █▃▄▄▃▁▅▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: Training loss 3e-05
wandb:     eval_loss 0.06605
wandb:      f1_score 0.91962
wandb:   global_step 1560
wandb:            lr 0.0
wandb:     precision 0.91529
wandb:        recall 0.92399
wandb:    train_loss 3e-05
wandb: 
wandb: 🚀 View run fanciful-disco-123 at: https://wandb.ai/tajak/NER/runs/r9t51vgg
wandb: ️⚡ View job at https://wandb.ai/tajak/NER/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkwODMzNTg1/version_details/v18
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230907_104235-r9t51vgg/logs
