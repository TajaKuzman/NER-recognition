wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/simpletransformers/ner/ner_model.py:483: UserWarning: The eval_df parameter has been renamed to eval_data. Using eval_df will raise an error in a future version.
  warnings.warn(
INFO:simpletransformers.ner.ner_model: Converting to features started.
['O', 'B-loc', 'B-org', 'B-per', 'I-per', 'B-deriv-per', 'I-org', 'I-loc', 'B-misc', 'I-misc', 'I-deriv-per']
(398681, 3) (51190, 3) (49764, 3)
     sentence_id      words labels
717            0      Kazna      O
718            0  medijskom      O
719            0     mogulu      O
720            0   obnovila      O
721            0   raspravu      O
wandb: Tracking run with wandb version 0.15.8
wandb: Run data is saved locally in /home/tajak/NER-recognition/wandb/run-20230823_102733-t612somb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-snow-101
wandb: ⭐️ View project at https://wandb.ai/tajak/NER
wandb: 🚀 View run at https://wandb.ai/tajak/NER/runs/t612somb
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05301531776308585, 'precision': 0.8066945606694561, 'recall': 0.8471001757469244, 'f1_score': 0.826403771967424}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.037756383307160246, 'precision': 0.8696383515559294, 'recall': 0.9086115992970123, 'f1_score': 0.8886978942844865}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.03860717800350308, 'precision': 0.877326565143824, 'recall': 0.9112478031634447, 'f1_score': 0.8939655172413792}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.04156683449455568, 'precision': 0.895716140199048, 'recall': 0.9094903339191565, 'f1_score': 0.9025506867233487}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.04295423220527544, 'precision': 0.8934319269247499, 'recall': 0.9024604569420035, 'f1_score': 0.8979234972677596}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.04789782098180715, 'precision': 0.8773624527509449, 'recall': 0.9178383128295254, 'f1_score': 0.8971440841743611}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05069155878256799, 'precision': 0.8878384185646756, 'recall': 0.9077328646748682, 'f1_score': 0.8976754290679991}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.051160182848180066, 'precision': 0.8937446443873179, 'recall': 0.9165202108963093, 'f1_score': 0.9049891540130153}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.055800008982847704, 'precision': 0.8988276161528441, 'recall': 0.9094903339191565, 'f1_score': 0.9041275387639223}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05819119494749298, 'precision': 0.8957795004306632, 'recall': 0.9138840070298769, 'f1_score': 0.9047411918225315}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.05983810437090528, 'precision': 0.8931955211024979, 'recall': 0.9112478031634447, 'f1_score': 0.9021313614615051}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06307983058787142, 'precision': 0.8964471403812825, 'recall': 0.9090509666080844, 'f1_score': 0.9027050610820245}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06237615501630229, 'precision': 0.8910338910338911, 'recall': 0.9125659050966608, 'f1_score': 0.901671369654873}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06430231305172608, 'precision': 0.8970588235294118, 'recall': 0.9112478031634447, 'f1_score': 0.90409764603313}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06594400542643045, 'precision': 0.8951647411210955, 'recall': 0.9191564147627417, 'f1_score': 0.9070019510080208}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06681238371528476, 'precision': 0.8951439621830684, 'recall': 0.9152021089630932, 'f1_score': 0.9050619161416469}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06834439709888201, 'precision': 0.9013900955690703, 'recall': 0.9116871704745168, 'f1_score': 0.906509392747925}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06703597283846101, 'precision': 0.899090515374621, 'recall': 0.9121265377855887, 'f1_score': 0.9055616139585606}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06729532896935773, 'precision': 0.895483870967742, 'recall': 0.9147627416520211, 'f1_score': 0.9050206476852858}
INFO:simpletransformers.ner.ner_model: Converting to features started.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.06828377246053614, 'precision': 0.8976683937823834, 'recall': 0.913444639718805, 'f1_score': 0.9054878048780489}
INFO:simpletransformers.ner.ner_model: Training of bert model complete. Saved to outputs/.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: Training loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     eval_loss ▄▁▁▂▂▃▄▄▅▆▆▇▇▇▇█████
wandb:      f1_score ▁▆▇█▇▇▇█████████████
wandb:   global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:            lr ▂▅████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:     precision ▁▆▆█▇▆▇▇██▇█▇███████
wandb:        recall ▁▇▇▇▆█▇█▇▇▇▇▇▇██▇▇█▇
wandb:    train_loss █▂▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: Training loss 9e-05
wandb:     eval_loss 0.06828
wandb:      f1_score 0.90549
wandb:   global_step 12380
wandb:            lr 0.0
wandb:     precision 0.89767
wandb:        recall 0.91344
wandb:    train_loss 3e-05
wandb: 
wandb: 🚀 View run northern-snow-101 at: https://wandb.ai/tajak/NER/runs/t612somb
wandb: ️⚡ View job at https://wandb.ai/tajak/NER/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkwODMzNTg1/version_details/v4
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230823_102733-t612somb/logs
